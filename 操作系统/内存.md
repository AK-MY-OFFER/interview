# 内存

## 问题

### 虚拟内存机制

* 为什么要虚拟内存，有什么用？
* 如何实现虚拟内存？
	* 硬件支持
	* 软件支持
* 谈谈请求页式存储管理？
* 谈谈请求段式存储管理？
* Linux 进程的内存分布长什么样？



### malloc 和 free

* malloc() 如何分配内存？

* malloc() 分配的是物理内存吗？

* malloc(1) 会分配多大内存？

* malloc() 为什么不全部使用 mmap 来分配内存？

* malloc() 为什么不全部使用 brk 来分配内存？

* malloc() 如果没有成功，可能什么原因，计算机会做什么？

* malloc() 是线程安全的吗 ?是可重入的吗？

* free() 释放内存，会归还给操作系统吗？

* free() 函数只传入一个内存地址，为什么会知道要释放多大的内存？

	

### 连续存储管理

* 静态分区分配是什么？有什么优缺点？
* 动态分区分配是什么？有什么优缺点？
* 什么是内部碎片？什么是外部碎片？各自有什么解决方法？



* 首次适应算法？
* 最佳适应算法？
* 最差适应算法？
* 邻近适应算法？



* Buddy 伙伴算法
* Slab 算法？

### 非连续存储管理

* 谈谈内存分段？
	* 什么是内存分段
	* 分段机制下，虚拟地址和物理地址是如何映射的？
	* 分段的优点
	* 分段的缺点及解决方案
* 谈谈内存分页？
	* 什么是内存分页
	* 分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？
	* 分页机制下，虚拟地址和物理地址是如何映射的？
	* 分页的优点
	* 分页的缺点及解决方案
	* 多级页表
	* TLB
* 谈谈段页式内存管理



### 页面置换算法

https://taifua.com/ostep-vm-beyondphys-policy.html

* 谈谈系统抖动

* 谈谈工作集

* 谈谈 Belady 异常

* 谈谈最佳页面置换算法 OPT
* 谈谈先进先出置换算法 FIFO
* 谈谈最近最少使用（最近最久未使用）的置换算法 LRU
* 谈谈近似 LRU
	* 周期置 0 ，这个周期长短如何确定。不同作业不同工作集？

* 谈谈如何优化传统的 LRU（预读、缓存失效）
* LRU-K https://zhuanlan.zhihu.com/p/78212068
* 谈谈时钟页面置换算法 CLOCK
* 谈谈最不常用置换算法 NFU
* 谈谈第二次机会页面置换算法 SC
* 谈谈老化算法
* 谈谈工作集页面置换算法
* 谈谈随机算法



TLB 命中和未命中会怎么样

什么时候 flush TLB？如何尽可能避免 TLB？

什么时候会发生 segment fault 数组越界一定会发生吗

mmap 的单位是多少

知道哪些 allocator（TCmalloc）

对于缺页中断这种情况，linux操作系统是如何减少页面的换入换出的呢，有哪些方式







## 回答

### 虚拟内存机制

#### 为什么要虚拟内存，有什么用？

单片机是没有操作系统的，所以每次写完代码，都需要借助工具把程序烧录进去，这样程序才能跑起来。

另外，**单片机的 CPU 是直接操作内存的「物理地址」**。

![img](https://img-blog.csdnimg.cn/019f1f0d2d30469cbda2b8fe2cf5e622.png)

在这种情况下，要想在内存中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃。

> 操作系统是如何解决这个问题呢？

这里关键的问题是这两个程序都引用了绝对物理地址，而这正是我们最需要避免的。

我们可以把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。

![进程的中间层](https://img-blog.csdnimg.cn/img_convert/298fb68e3da94d767b02f2ed81ebf2c4.png)

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。

操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：

![img](https://img-blog.csdnimg.cn/72ab76ba697e470b8ceb14d5fc5688d9.png)

**虚拟内存的作用**

- 第一，虚拟内存可以使得进程的运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。
- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。



#### 谈谈请求页式存储管理？





#### Linux 进程的内存分布长什么样？

那么，Linux 操作系统采用了哪种方式来管理内存呢？

> 在回答这个问题前，我们得先看看 Intel 处理器的发展历史。

早期 Intel 的处理器从 80286 开始使用的是段式内存管理。但是很快发现，光有段式内存管理而没有页式内存管理是不够的，这会使它的 X86 系列会失去市场的竞争力。因此，在不久以后的 80386 中就实现了页式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了页式内存管理。

但是这个 80386 的页式内存管理设计时，没有绕开段式内存管理，而是建立在段式内存管理的基础上，这就意味着，**页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。**

由于此时由段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由页式内存管理将线性地址映射成物理地址。

![img](https://img-blog.csdnimg.cn/bc0aaaf379fc4bc8882efd94b9052b64.png)

这里说明下逻辑地址和线性地址：

- 程序所使用的地址，通常是没被段式内存管理映射的地址，称为逻辑地址；
- 通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址；

逻辑地址是「段式内存管理」转换前的地址，线性地址则是「页式内存管理」转换前的地址。

> 了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采用了什么方式管理内存？

**Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制**。

这主要是上面 Intel 处理器发展历史导致的，因为 Intel X86 CPU 一律对程序中使用的地址先进行段式映射，然后才能进行页式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。

但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作用。也就是说，“上有政策，下有对策”，若惹不起就躲着走。

**Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。**

> 我们再来瞧一瞧，Linux 的虚拟地址空间是如何分布的？

在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：

![img](https://img-blog.csdnimg.cn/3a6cb4e3f27241d3b09b4766bb0b1124.png)

通过这里可以看出：

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

再来说说，内核空间与用户空间的区别：

- 进程在用户态时，只能访问用户空间内存；
- 只有进入内核态后，才可以访问内核空间的内存；

虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

![img](https://img-blog.csdnimg.cn/48403193b7354e618bf336892886bcff.png)

接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。

我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系：

![虚拟内存空间划分](https://img-blog.csdnimg.cn/img_convert/b4f882b9447760ce5321de109276ec23.png)

通过这张图你可以看到，用户空间内存，从**低到高**分别是 6 种不同的内存段：

- 程序文件段（.text），包括二进制可执行代码；
- 已初始化数据段（.data），包括静态常量；
- 未初始化数据段（.bss），包括未初始化的静态变量；
- 堆段，包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关 (opens new window)](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；

上图中的内存布局可以看到，程序文件段（.text）下面还有一段内存空间的（灰色部分），这一块区域是「保留区」，之所以要有保留区这是因为在大多数的系统里，我们认为比较小数值的地址不是一个合法地址，例如，我们通常在 C 的代码里会将无效的指针赋值为 NULL。因此，这里会出现一段不可访问的内存保留区，防止程序因为出现 bug，导致读或写了一些小内存地址的数据，而使得程序跑飞。

在这 7 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。



**源码视角**

https://blog.csdn.net/FF_programming/article/details/120963212

**1、task_struct**

每个进程在内核中都有一个进程控制块(PCB)来维护进程相关的信息，Linux内核的进程控制块是task_struct[结构体](https://so.csdn.net/so/search?q=结构体&spm=1001.2101.3001.7020)。

- 位置：<include\linux\sched.h> - 593行
- 部分代码如下：

```c
struct task_struct
{
    /*...*/
	struct mm_struct		*mm;
	struct mm_struct		*active_mm;

	/* Per-thread vma caching: */
	struct vmacache			vmacache;
    /*...*/
}

```

* `*mm`
mm为mm_struct类型的指针，指向mm_struct结构，对于普通的用户进程来说mm字段指向他的虚拟地址空间的用户空间部分，对于内核线程来说这部分为NULL。
* `*active_mm`
  对于匿名进程，也可以理解为内核线程，它的mm字段为NULL，表示没有内存地址空间，可也并不是真正的没有，这是因为所有进程关于内核的映射都是一样的，内核线程可以使用任意进程的地址空间。
  **内核schedule在进程上下文切换的时候，会根据task->mm判断即将调度的进程是用户进程还是内核线程。**
  由于内核线程之前可能是任何用户层进程在执行，故用户空间部分的内容本质上是随机的，内核线程决不能修改其内容，故将mm设置为NULL。所以对于内核线程来说task_struct->mm == NULL，而task_struct->active_mm为某个进程的mm。
  如果切换出去的是用户进程，内核将原来进程的mm存放在新内核线程的active_mm中，因为某些时候内核必须知道用户空间当前包含了什么。
  而当内核线程离开的时候，会把借用的地址空间将会返还给原来的进程，并且清除这一字段。
* `vmcacahe`
  vmacache结构体定义在<include\linux\mm_types_task.h>的34行，如下：

```c
#define VMACACHE_BITS 2
#define VMACACHE_SIZE (1U << VMACACHE_BITS)

struct vmacache {
  u64 seqnum;
  struct vm_area_struct *vmas[VMACACHE_SIZE];
};

```

可以看到它是一个vm_area_struct类型的数组指针，表示指向几段vma。
这是由于为了提高vma的查找速度，由传统的链表式改为了红黑树管理，然而某些进程地址空间中的vma数量众多，而查找vma又是内核中非常频繁的操作，所以为了进一步加快查找速度，内核采取了一种缓存方式，就是把最近访问的几个vma保存起来。
VMACACHE_SIZE为左移2位的一个无符号整形数字，这里感觉大概是保存了4个最近访问的vma。



**2、mm_struct**

每一个进程都会有唯一的mm_struct结构体

- 位置：<include\linux\mm_types.h> - 340行
- 部分代码如下：

```c
struct mm_struct
{
    /*...*/
    struct vm_area_struct *mmap;		/* list of VMAs */
	struct rb_root mm_rb;
	u64 vmacache_seqnum;                /* per-thread vmacache */
    unsigned long mmap_base;	/*映射基地址*/
	unsigned long mmap_legacy_base;	/*不是很明白这里*/
    unsigned long task_size;	/*该进程能够vma使用空间大小*/
	unsigned long highest_vm_end;	/*该进程能够使用的vma结束地址*/
	pgd_t * pgd;
    atomic_t mm_users;
    atomic_t mm_count;
    int map_count;			/* vma的总个数 */
    unsigned long total_vm;	   /* 映射的总页面数*/
    /*...*/
    unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;
    /*...*/

```

* *mmap
	该进程内已经使用的进程虚拟空间vm_are_struct结构，以双链表形式存储该指针指向vma链表的头节点。
* mm_rb
	红黑树组织形式，用于快速查找。
	rb_boot结构体定义在<include\linux\rbtree.h>的43行

```c
struct rb_root {
  struct rb_node *rb_node;
};
```


rb_node结构体定义在<include\linux\rbtree.h>的36行

```c
struct rb_node {
  unsigned long  __rb_parent_color;
  struct rb_node *rb_right;
  struct rb_node *rb_left;
  } __attribute__((aligned(sizeof(long))));
```


可以看到该数据结构包含了父节点颜色、左子树指针、右子树指针等字段

* vmacache_seqnum
	vma的缓存策略所使用的字段，表示每个进程都有的几个缓存vma，对应task_struct->vmacache->seqnum。
* *pgd
	**该进程页目录表，把新进程mm_struct的PGD字段填充到CR3中就完成了页表的切换。**
* mm_users、mm_count
	为了支持mm与active_mm，在这里设置了两个计数器，mm_users主要用来记录共用此命名空间的线程数量，mm_count则主要记录对此mm_struct结构体的引用情况。
	也就是说mm_users表示有多少真正使用地址空间的进程，当copy_mm函数在创建进程的时候，遇到CLONE_VM字段的话就再到需要让父子进程共享同一个地址空间，这时会把mm_users加1。而mm_count则表示有多少内核线程引用了这个地址空间。
	当mm_users为0时就会释放mm_count，当mm_count为0时结构体mm_struct将会被释放。



**3、vm_area_struct**
内核每次为用户空间中分配一个空间使用时，都会生成一个vm_area_struct结构用于记录跟踪分配情况，一个vm_area_struct就代表一段虚拟内存空间。

位置：<include\linux\mm_types.h> - 264行
全部代码如下：

```c
struct vm_area_struct {
	unsigned long vm_start; //虚存区起始
	unsigned long vm_end;   //虚存区结束
	struct vm_area_struct *vm_next, *vm_prev;   //前后指针
	struct rb_node vm_rb;   //红黑树中的位置
	unsigned long rb_subtree_gap;
	struct mm_struct *vm_mm;    //所属的 mm_struct
	pgprot_t vm_page_prot;      
	unsigned long vm_flags;     //标志位
	struct {
		struct rb_node rb;
		unsigned long rb_subtree_last;
	} shared;	
	struct list_head anon_vma_chain;
	struct anon_vma *anon_vma;
	const struct vm_operations_struct *vm_ops;  //vma对应的实际操作
	unsigned long vm_pgoff;     //文件映射偏移量
	struct file * vm_file;      //映射的文件
	void * vm_private_data;     //私有数据
	atomic_long_t swap_readahead_info;
#ifndef CONFIG_MMU
	struct vm_region *vm_region;	/* NOMMU mapping region */
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
#endif
	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
} __randomize_layout;

```

***vm_ops**
**vm_operations_struct**结构体定义在<include\linux\mm.h>的393行

```c
struct vm_operations_struct {
  void (*open)(struct vm_area_struct * area);
  void (*close)(struct vm_area_struct * area);
  int (*split)(struct vm_area_struct * area, unsigned long addr);
  int (*mremap)(struct vm_area_struct * area);
  vm_fault_t (*fault)(struct vm_fault *vmf);
  vm_fault_t (*huge_fault)(struct vm_fault *vmf,
  		enum page_entry_size pe_size);
  void (*map_pages)(struct vm_fault *vmf,
  		pgoff_t start_pgoff, pgoff_t end_pgoff);
  unsigned long (*pagesize)(struct vm_area_struct * area);

  /* notification that a previously read-only page is about to become
   * writable, if an error is returned it will cause a SIGBUS */
  vm_fault_t (*page_mkwrite)(struct vm_fault *vmf);

  /* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */
  vm_fault_t (*pfn_mkwrite)(struct vm_fault *vmf);

  /* called by access_process_vm when get_user_pages() fails, typically
   * for use by special VMAs that can switch between memory and hardware
   */
  int (*access)(struct vm_area_struct *vma, unsigned long addr,
  	      void *buf, int len, int write);

  /* Called by the /proc/PID/maps code to ask the vma whether it
   * has a special name.  Returning non-NULL will also cause this
   * vma to be dumped unconditionally. */
  const char *(*name)(struct vm_area_struct *vma);

  #ifdef CONFIG_NUMA
  /*
   * set_policy() op must add a reference to any non-NULL @new mempolicy
   * to hold the policy upon return.  Caller should pass NULL @new to
   * remove a policy and fall back to surrounding context--i.e. do not
   * install a MPOL_DEFAULT policy, nor the task or system default
   * mempolicy.
   */
  int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);

  /*
   * get_policy() op must add reference [mpol_get()] to any policy at
   * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
   * in mm/mempolicy.c will do this automatically.
   * get_policy() must NOT add a ref if the policy at (vma,addr) is not
   * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.
   * If no [shared/vma] mempolicy exists at the addr, get_policy() op
   * must return NULL--i.e., do not "fallback" to task or system default
   * policy.
   */
  struct mempolicy *(*get_policy)(struct vm_area_struct *vma,
  				unsigned long addr);
  #endif
  /*
   * Called by vm_normal_page() for special PTEs to find the
   * page for @addr.  This is useful if the default behavior
   * (using pte_page()) would not find the correct page.
   */
  struct page *(*find_special_page)(struct vm_area_struct *vma,
  				  unsigned long addr);
  };


```

vm_operations结构中包含的是函数指针，其中open为虚存区的打开，close用于虚存区的关闭，另外还有split、mremap、acess等操作。

shared
对于具有地址空间和后备存储的区域，链接到地址空间的i_mmap间隔树。

rb_subtree_gap
该vma子树最大可用内存间隔（以字节为单位），具体为与vma和vma->vm_prev之间，或子vma和它的vma->vm_prev之间的内存间隙，有助于找到大小合适的空闲区域。

vm_flags
描述该区域的一段标志，包括其读写属性等，采用bit位方式，状态标志位定义在<include\linux\mm.h>文件173行中：

```c
  #define VM_NONE		0x00000000  //无标志

  #define VM_READ		0x00000001	//可读
  #define VM_WRITE	0x00000002  //可写
  #define VM_EXEC		0x00000004  //可操作
  #define VM_SHARED	0x00000008  //允许被多个线程访问

  /* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */
  #define VM_MAYREAD	0x00000010	//允许设置可读
  #define VM_MAYWRITE	0x00000020	//允许设置可写  
  #define VM_MAYEXEC	0x00000040	//允许设置可操作
  #define VM_MAYSHARE	0x00000080	//允许设置可共享

  #define VM_GROWSDOWN	0x00000100	//向低地址增长
  #define VM_UFFD_MISSING	0x00000200	
  #define VM_PFNMAP	0x00000400	
  #define VM_DENYWRITE	0x00000800	//不允许写入
  #define VM_UFFD_WP	0x00001000	
  #define VM_LOCKED	0x00002000	//VMA被锁定，不会被交换到交换分区
  #define VM_IO           0x00004000	//该VMA用于IO 映射

  #define VM_SEQ_READ	0x0000800	//应用程序会顺序读该VMA的内容
  #define VM_RAND_READ	0x00010000	//应用程序会随机读取该VMA内存
  #define VM_DONTCOPY	0x00020000	//fork时不会复制该VMA
  #define VM_DONTEXPAND	0x00040000	
  #define VM_LOCKONFAULT	0x00080000	//当处于page fault时锁定该VMA 对应的物理页
  #define VM_ACCOUNT	0x00100000	
  #define VM_NORESERVE	0x00200000	//不做保留
  #define VM_HUGETLB	0x00400000	//huge TLB page对应的VM
  #define VM_SYNC		0x00800000	//page fault时同步映射
  #define VM_ARCH_1	0x01000000	
  #define VM_WIPEONFORK	0x02000000	//不会从父进程相应的VMA中复制页表到子进程的VMA中
  #define VM_DONTDUMP	0x04000000	//VMA不会被包含到core dump中

  #ifdef CONFIG_MEM_SOFT_DIRTY
  # define VM_SOFTDIRTY	0x08000000	
  #else
  # define VM_SOFTDIRTY	0
  #endif

  #define VM_MIXEDMAP	0x10000000	
  #define VM_HUGEPAGE	0x20000000	
  #define VM_NOHUGEPAGE	0x40000000	
  #define VM_MERGEABLE	0x80000000	

```

- **vm_page_prot**
	vm_flags代表VMA的状态位，但是相应的状态位最后要转化成实际内存的下发到硬件状态位，而pgprot则是代表实际物理页状态位。



**4、关系图**![在这里插入图片描述](https://img-blog.csdnimg.cn/703ddd089fd643f481d7d41c2a91531e.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBARkZfcHJvZ3JhbW1pbmc=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

CSAPP 虚拟内存章节看一下

linux内核使用vm_area_struct结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制都不同，因此一个进程使用多个vm_area_struct结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct结构使用链表或者树形结构链接，方便进程快速访问，如下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/img_convert/33f6c2014042fc8356990f33df47a22f.png)

### malloc 和 free

#### malloc() 如何分配内存？

实际上，malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。

- 方式一：通过 brk() 系统调用从堆分配内存
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；

方式一实现的方式很简单，就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/brk%E7%94%B3%E8%AF%B7.png)

方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/mmap%E7%94%B3%E8%AF%B7.png)

> 什么场景下 malloc() 会通过 brk() 分配内存？又是什么场景下通过 mmap() 分配内存？

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
- 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

注意，不同的 glibc 版本定义的阈值也是不同的。



man 手册

> 通常，malloc () 从堆中分配内存，并调整使用sbrk(2)根据需要设置堆的大小。当分配
> 大于MMAP_THRESHOLD字节的内存块时，glibc malloc () 实现使用mmap(2)将内存分配为私有匿名映射。  MMAP_THRESHOLD默认为 128 kB ，但可以使用mallopt(3)进行调整。在 Linux 4.7 之前，使用 mmap(2)执行的分配不受 RLIMIT_DATA资源限制的影响；自 Linux 4.7 起，此限制也适用于使用 mmap(2) 执行的分配。



#### malloc() 分配的是物理内存吗？

不是的，**malloc() 分配的是虚拟内存**。

如果分配后的虚拟内存没有被访问的话，虚拟内存是不会映射到物理内存的，这样就不会占用物理内存了。

只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。



#### malloc(1) 会分配多大内存？

malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。

具体会预分配多大的空间，跟 malloc 使用的内存管理器有关系，我们就以 malloc 默认的内存管理器（Ptmalloc2）来分析。

接下里，我们做个实验，用下面这个代码，通过 malloc 申请 1 字节的内存时，看看操作系统实际分配了多大的内存空间。

```c
#include <stdio.h>
#include <malloc.h>

int main() {
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
  
  //申请1字节的内存
  void *addr = malloc(1);
  printf("此1字节的内存起始地址：%x\n", addr);
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
 
  //将程序阻塞，当输入任意字符时才往下执行
  getchar();

  //释放内存
  free(addr);
  printf("释放了1字节的内存，但heap堆并不会释放\n");
  
  getchar();
  return 0;
}
```

执行代码（**先提前说明，我使用的 glibc 库的版本是 2.17**）：

![图片](https://img-blog.csdnimg.cn/img_convert/080ee187c8c92db45092b6688774e8da.png)

我们可以通过 /proc//maps 文件查看进程的内存分布情况。我在 maps 文件通过此 1 字节的内存起始地址过滤出了内存地址的范围。

```shell
[root@xiaolin ~]# cat /proc/3191/maps | grep d730
00d73000-00d94000 rw-p 00000000 00:00 0                                  [heap]
```

这个例子分配的内存小于 128 KB，所以是通过 brk() 系统调用向堆空间申请的内存，因此可以看到最右边有 [heap] 的标识。

可以看到，堆空间的内存地址范围是 00d73000-00d94000，这个范围大小是 132KB，也就说明了 **malloc(1) 实际上预分配 132K 字节的内存**。

可能有的同学注意到了，程序里打印的内存起始地址是 `d73010`，而 maps 文件显示堆内存空间的起始地址是 `d73000`，为什么会多出来 `0x10` （16字节）呢？这个问题，我们先放着，后面会说。



#### malloc() 为什么不全部使用 mmap 来分配内存？

因为向操作系统申请内存，是要通过系统调用的，执行系统调用是要进入内核态的，然后在回到用户态，运行态的切换会耗费不少时间。

所以，申请内存的操作应该避免频繁的系统调用，如果都用 mmap 来分配内存，等于每次都要执行系统调用。

另外，因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断。

也就是说，**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**。

为了改进这两个问题，malloc 通过 brk() 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中。

**等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗**。



#### mmap 实现原理？



#### malloc() 为什么不全部使用 brk 来分配内存？

前面我们提到通过 brk 从堆空间分配的内存，并不会归还给操作系统，那么我们那考虑这样一个场景。

如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。

![图片](https://img-blog.csdnimg.cn/img_convert/75edee0cb75450e7987a8a482b975bda.png)

但是如果下次申请的内存大于 30k，没有可用的空闲内存空间，必须向 OS 申请，实际使用内存继续增大。

因此，随着系统频繁地 malloc 和 free ，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的。

所以，malloc 实现中，充分考虑了 brk 和 mmap 行为上的差异及优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间。



#### malloc() 如果没有成功，可能什么原因，计算机会做什么？

*  内存不足。 
*  在前面的程序中出现了内存的越界访问，导致malloc()分配函数所涉及的一些信息被破坏。下次再使用malloc()函数申请内存就会失败，返回空指针NULL(0)。

查看方式：

1、内存不足，使用free命令查看当前还有多少内存，看是否合理，之前是否有内存泄漏等。

2、按照流程查看malloc失败前的几次malloc、memcpy或字符串拷贝等，查看是否有内存越界。



man 手册

>  默认情况下，Linux 遵循乐观的内存分配策略。这意味着当malloc () 返回非 NULL 时，不能保证内存确实可用。如果 发现系统内存不足，OOM 杀手将杀死一个或多个进程。更多信息请参见proc(5)中/proc/sys/vm/overcommit_memory和 /proc/sys/vm/oom_adj的描述，以及 Linux 内核源文件 Documentation/vm/overcommit-accounting.rst。



#### malloc() 是线程安全的吗 ?是可重入的吗？

**线程安全**

man 手册

> 为了避免多线程应用程序中的损坏，内部使用互斥锁来保护这些函数使用的内存管理数据结构。在线程同时分配和释放内存的多线程应用程序中，可能存在对这些互斥体的争用。为了在多线程应用程序中可伸缩地处理内存分配，如果检测到互斥争用，glibc 会创建
> 额外的内存分配区域。每个 arena 都是由系统内部分配的大内存区域（使用brk(2)或mmap(2)），并且使用自己的互斥锁进行管理。



https://blog.csdn.net/qq_40334837/article/details/96423711

**malloc 可重入吗？**

在man手册中，与系统调用有关的函数都会说明该函数是否线程安全，所以这也是我们写代码需要关注的，而线程安全与函数是否可重入有很大关系，**函数可重入一定是线程安全的，线程安全不一定是可重入函数，比如maloc使用递归锁实现了线程安全，但它是不可重入函数**，所以不可重入函数可以通过内核锁实现线程安全（锁是系统调用，所以工作在内核态，也叫内核锁），还有很多函数也是这样实现线程安全的



**信号**

1、基本概念

软中断信号（signal，又简称为信号）用来**通知进程发生了异步事件**。进程之间可以互相通过系统调用kill发送软中断信号。内核也可以因为内部事件而给进程发送信号，通知进程发生了某个事件。注意，信号只是用来通知某进程发生了什么事件，并不给该进程传递任何数据。

收到信号的进程对各种信号有不同的处理方法。处理方法可以分为三类：

* 第一种是类似中断的处理程序，对于需要处理的信号，进程可以指定处理函数，由该函数来处理。
* 第二种方法是，忽略某个信号，对该信号不做任何处理，就象未发生过一样。
* 第三种方法是，对该信号的处理保留系统的默认值，这种缺省操作，对大部分的信号的缺省操作是使得进程终止。进程通过系统调用signal来指定进程对某个信号的处理行为。



**内核对信号的基本处理方法**

**内核处理一个进程收到的信号的时机是在一个进程从内核态返回用户态时。所以，当一个进程在内核态下运行时，软中断信号并不立即起作用，要等到将返回用户态时才处理**，比如线程中系统调用malloc申请内存，所以从用户态进入了内核态，现在信号发生，这时操作系统会从内核态跳转到用户态执行signal函数（signal函数本身是应用层的一行代码需要被运行，属于用户态，当执行到绑定的函数时，函数内部有系统调用函数，然后进入内核态）。进程收到一个要捕捉的信号，那么进程从内核态返回用户态时执行用户定义的函数，而且执行用户定义的函数的方法很巧妙（这个函数时signal函数绑定的那个处理信号函数，比如里面出现malloc系统调用），内核是在用户栈上创建一个新的层，该层中将返回地址的值设置成用户定义的处理函数的地址，这样进程从内核返回弹出栈顶时就返回到用户定义的函数处，从函数返回再弹出栈顶时，才返回原先进入内核的地方（线程中调用malloc所在内核执行处）。这样做的原因是用户定义的处理函数不应该在内核态下执行，所以一般在信号处理函数中，最好仅仅用来打印一条信息，然后使用longjmp或者exit退出。

![img](https://img-blog.csdnimg.cn/20190718103716258.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzM0ODM3,size_16,color_FFFFFF,t_70)

综上所述，如果信号被捕获，执行点从内核态返回用户态，在返回时，如果发现待执行进程存在被触发的signal，那么在离开内核态之后（也就是将CPU切换到用户模式），执行用户进程为该signal绑定的signal处理函数，从这一点上看，signal处理函数是在用户进程上下文中执行的。当执行完signal处理函数之后，再返回到用户进程被中断或者system call（软中断或者指令陷阱）打断的地方。



**如果signal处理函数使用系统调用，比如malloc，free**

信号处理函数中只能调用可重入函数，而不能调用不可重入函数。进程捕捉到信号并对其进行处理时，正在执行的正常指令序列就被信号处理程序临时中断，它首先执行该信号处理函数中的指令。如果从信号处理程序返回，则继续执行在捕捉到信号时正在执行的正常指令序列（这类似于发生硬件中断时所做的）。但在信号处理函数中，不能判断捕捉到信号时线程执行到何处。

 信号处理函数默认情况下是在进程的主线程调用的，这种情况下使用不可重入函数，有可能会造成不可预知的错误。比如调用了malloc函数，为了保证malloc是线程安全的，所以内部使用了锁，根据malloc中锁的不同处理方式，分别可能会导致以下情况的发生：

1)  如果是普通锁，在主线程中malloc函数获取锁之后被signal中断，在signal处理函数中继续调用malloc，因为主线程中的malloc已经获取到了锁，signal处理函数只能等待锁释放，而主线程中的malloc函数正在等待signal处理函数返回后继续执行，这样就造成了锁死；

2)  如果是递归锁，那么signal处理函数中的malloc函数获取锁后进行内存分配，因为上次的malloc操作还没完，可能成会造成内存数据混乱。




就定时而言，可不直接使用singal alarm，而使用posix定时器，通过通知线程的方式，将定时处理函数放到单独的线程中来处理。



**内核线程调度系统对malloc的处理方法**

多线程之前使用malloc是安全的，虽然它不可重入，但是用锁实现了

1)  如果是普通锁，A线程获取堆栈锁后，B线程必须等待A线程执行完成后，释放锁，然后B线程malloc才能申请内存。

2)  如果是递归锁，内核调度系统在线程之间调度时，如果线程A的malloc函数一旦开始了申请，它就不会交出CPU，而是等malloc完成后，才会根据情况是否交出CPU，如果没有特别重要的处理，调度器就会跳转到线程B中执行，如果B线程执行到malloc，同理。（调度器如何工作等我搞明白再修正这




**代码例子**

https://zhuanlan.zhihu.com/p/371222679

```c
#include <stdio.h>
#include <unistd.h>
#include <signal.h>
#include <string.h>
#include <stdlib.h>

void sig_handler(int signum)
{
    printf("\nInside handler function\n");
    char *p = NULL;
    p = (char *)malloc(sizeof(char) * 1024);
    if (NULL == p)
    {
        return;
    }
    free(p);
}


int main(int argc, char **argv)
{
    char *p = NULL;
    signal(SIGINT, sig_handler); // 初始化信号处理函数
    
    while (1)
    {
        p = (char *)malloc(sizeof(char) * 1024);
        if (NULL == p)
        {
            continue;
        }
        free(p);
    }

    return 0;
}
```

编译上述代码：

```text
gcc -g testsigint.c -o testsigint
```

运行代码，不停的按ctrl+c（要亿点点运气）

```text
[root@root]# ./testsigint
^C
Inside handler function
^C
Inside handler function
^C
Inside handler function
^C
Inside handler function
^C^C^C^C^C^C
```

当出现多个^C的时候，证明该进程已经锁死了。

新开一个窗口，获取该进程的堆栈打印。

```text
[root@root]# pstack `pidof testsigint`
#0  0x00007faae50275cc in __lll_lock_wait_private () from /lib64/libc.so.6
#1  0x00007faae4fa3b12 in _L_lock_16654 () from /lib64/libc.so.6
#2  0x00007faae4fa0753 in malloc () from /lib64/libc.so.6
#3  0x0000000000400634 in sig_handler (signum=2) at testsigint.c:11
#4  <signal handler called>
#5  0x00007faae4f9bf1b in _int_free () from /lib64/libc.so.6
#6  0x0000000000400699 in main (argc=1, argv=0x7fff29e79aa8) at testsigint.c:32
[root@localhost ~]# pstack `pidof testsigint`
#0  0x00007faae50275cc in __lll_lock_wait_private () from /lib64/libc.so.6
#1  0x00007faae4fa3b12 in _L_lock_16654 () from /lib64/libc.so.6
#2  0x00007faae4fa0753 in malloc () from /lib64/libc.so.6
#3  0x0000000000400634 in sig_handler (signum=2) at testsigint.c:11
#4  <signal handler called>
#5  0x00007faae4f9bf1b in _int_free () from /lib64/libc.so.6
#6  0x0000000000400699 in main (argc=1, argv=0x7fff29e79aa8) at testsigint.c:32
```

可以看到进程挂死在了malloc和free结对的地方，肯定是有内部资源互斥了。

在进程通过信号产生软中断进入用户自定义的信号处理函数，内核会保存和恢复进程的上下文，然而恢复的上下文仅限于返回地址，cpu寄存器等之类的少量上下文，而用户态函数内部使用的一些全局变量、静态变量，锁等并不在保护之列，所以如果这些值在信号处理函数发生了改变，那么当函数回到用户原本的上下文继续执行时，其结果就不可预料了。

比如上面的malloc，将如一个进程此时正在执行malloc分配堆空间，此时程序捕捉到信号发生中断，执行信号处理程序中恰好也有一个malloc，这样就会对进程的环境造成破坏，因为malloc通常为它所分配的存储区维护一个链接表和一些锁，插入执行信号处理函数时，进程可能正在对这张表进行操作，而信号处理函数的调用可能回去获取同一把锁，也可能回去修改内存里面的值，这样造成的结果是不可预期的。







#### free() 释放内存，会立即归还给操作系统吗？

我们在上面的进程往下执行，看看通过 free() 函数释放内存后，堆内存还在吗？

![图片](https://img-blog.csdnimg.cn/img_convert/1a9337f8f6b83fbc186f257511b5ce67.png)

从下图可以看到，通过 free 释放内存后，堆内存还是存在的，并没有归还给操作系统。

![图片](https://img-blog.csdnimg.cn/img_convert/2b8f63892830553ec04c5f05f336ae8b.png)

这是因为与其把这 1 字节释放给操作系统，不如先缓存着放进 malloc 的内存池里，当进程再次申请 1 字节的内存时就可以直接复用，这样速度快了很多。

当然，当进程退出后，操作系统就会回收进程的所有资源。

上面说的 free 内存后堆内存还存在，是针对 malloc 通过 brk() 方式申请的内存的情况。

如果 malloc 通过 mmap 方式申请的内存，free 释放内存后就会归归还给操作系统。

我们做个实验验证下， 通过 malloc 申请 128 KB 字节的内存，来使得 malloc 通过 mmap 方式来分配内存。

```c
#include <stdio.h>
#include <malloc.h>

int main() {
  //申请1字节的内存
  void *addr = malloc(128*1024);
  printf("此128KB字节的内存起始地址：%x\n", addr);
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());

  //将程序阻塞，当输入任意字符时才往下执行
  getchar();

  //释放内存
  free(addr);
  printf("释放了128KB字节的内存，内存也归还给了操作系统\n");

  getchar();
  return 0;
}
```

执行代码：

![图片](https://img-blog.csdnimg.cn/img_convert/500fdc021d956f60963f308760f511d0.png)

查看进程的内存的分布情况，可以发现最右边没有 [head] 标志，说明是通过 mmap 以匿名映射的方式从文件映射区分配的匿名内存。

![图片](https://img-blog.csdnimg.cn/img_convert/501f458b8d35abe5e378a0f14c667797.png)

然后我们释放掉这个内存看看：

![图片](https://img-blog.csdnimg.cn/img_convert/fcdbe91cc03b6a2f6e93dd1971d1b438.png)

再次查看该 128 KB 内存的起始地址，可以发现已经不存在了，说明归还给了操作系统。

![图片](https://img-blog.csdnimg.cn/img_convert/3f63c56b131d92806b5aabca29d33a38.png)

对于 「malloc 申请的内存，free 释放内存会归还给操作系统吗？」这个问题，我们可以做个总结了：

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。

#### free() 函数只传入一个内存地址，为什么会知道要释放多大的内存？

还记得，我前面提到， malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节吗？

这个多出来的 16 字节就是保存了该内存块的描述信息，比如有该内存块的大小。

![图片](https://img-blog.csdnimg.cn/img_convert/cb6e3ce4532ff0a6bfd60fe3e52a806e.png)

这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。



### 连续存储管理

#### 静态分区分配是什么？有什么优缺点？

优点：简单，要求的硬件支持少，软件算法也简单

缺点：

* 内存利用率低，产生内部碎片
* 尺寸和分区数量难以确定
* 分区总数固定，限制了并发执行的程序数量



#### 动态分区分配是什么？有什么优缺点？

不预先划分内存，在程序装入内存时，根据进程的大小动态地建立分区，并使得分区的大小正好适合进程的需要，因此系统中分区的大小和数目是可变的。

在进程装入或换入主存时，如果内存中有多个足够大的空闲块，操作系统必须确定分配哪个内存块给进程使用，这就是动态分区的分配策略



优点：

*  没有内部碎片
*  能较有效利用内存空间，提高了多道程序系统对内存的共享

缺点：

* 容易产生外部碎片问题，为解决外部碎片问题，需要采用动态重定位，增加了计算机硬件成本，而紧凑工作又要花费大量处理机时间



#### 什么是内部碎片？什么是外部碎片？有什么解决方法？

**内部碎片就是已经被分配出去（能明确指出属于哪个进程）却不能被利用的内存空间；**

内部碎片是处于区域内部或页面内部的存储块。占有这些区域或页面的进程并不使用这个存储块。而在进程占有这块存储块时，系统无法利用它。直到进程释放它，或进程结束时，系统才有可能利用这个存储块。



**外部碎片指的是还没有被分配出去（不属于任何进程），但由于太小了无法分配给申请内存空间的新进程的内存空闲区域。**

外部碎片是出于任何已分配区域或页面外部的空闲存储块。这些存储块的总和可以满足当前申请的长度要求，但是由于它们的地址不连续或其他原因，使得系统无法满足当前申请



**解决外部碎片 --> 紧缩：**

移动内存中的进程，将碎片集中起来，重新构成大的内存块。需要运行时的动态重定位，费时。

紧缩方向：

* 向一头紧缩
* 向两头紧缩。

紧缩时机：

* 在释放分区时，如果不能与空闲分区合并，则立刻进行紧缩。**好处是不存在外部碎片，坏处是费时。**
* 在内存分配时，如果剩余的空闲空间总量能满足要求但没有一个独立的空闲块能满足要求，则进行紧缩。**好处是减少紧缩次数**。





#### 首次适应算法？

思想：从头到尾寻找合适的分区



将所有空闲分区**按照地址递增的次序链接**，在申请内存分配时，从链首开始查找，**将满足需求的第一个**空闲分区分配给作业



优点：

* 综合看，首次适应算法性能最好。**算法开销小**，回收分区后，一般不需要对空闲分区队列重新排序
* 分区集中在内存的前部，大内存留在后面，便于释放后的合并



#### 最佳适应算法？

思想：优先使用更小的分区，以保留更多的大分区



将所有空闲分区按照**从小到大**的顺序形成空闲分区链，在申请内存分配时，总是把**满足需求的、最小的**空闲分区分配给作业



优点：会有更多的大分区被保留下来，更能满足大进程需求

缺点

* 会产生很多太小的、难以利用的外部碎片，需要频繁紧缩
* **算法开销大**，回收分区后可能需要对空闲分区队列重新排序



#### 最差适应算法？

思想：优先使用更大的分区，以防止产生太小的不可用碎片



将所有的空闲分区按照**从大到小**的顺序形成空闲分区链，在申请内存分配时，总是把**满足需求的、最大的**空闲分区分配给作业



优点：优先使用更大的分区，以防止产生太小的不可用碎片

缺点

* 大分区容易被用完，不利于大进程
* **算法开销大**，回收分区后可能需要对空闲分区队列重新排序



#### 邻近适应算法？（循环首次适应算法）

思想：由首次适应算法演变而来，每次从上次查找结束的位置开始查找



将所有空闲分区**按照地址递增的次序链接**（可排列成循环链表），在申请内存分配时，**总是从上次找到的空闲分区的下一个空闲分区开始查找**，将满足需求的第一个空闲分区分配给作业



优点：不用每次都从低地址的小分区开始检索**算法开销小**（原因同首次适应算法）

缺点：会使高地址的大分区也被用完





#### Buddy 伙伴算法？

伙伴算法：将动态分区的大小限定为 2^k  字节，分割方式限定为平分，分区就会变得较为规整，分割与合并会更容易，可以减少一些外部碎片。平分后的两块互称伙伴。

![img](https:////upload-images.jianshu.io/upload_images/5346502-c766de1d4facb6ee.png?imageMogr2/auto-orient/strip|imageView2/2/w/653/format/webp)

分配时可能要多次平分，释放时可能要多次合并。举例：

![img](https:////upload-images.jianshu.io/upload_images/5346502-937492787301e74a.png?imageMogr2/auto-orient/strip|imageView2/2/w/565/format/webp)

**伙伴的概念**，满足以下三个条件的称为伙伴：

* 两个块大小相同；
* 两个块地址连续；
* 两个块必须是同一个大块中分离出来的；



**关于位图**
Linux内核伙伴算法中每个order 的位图都表示所有的空闲块，位图的某位对应于两个伙伴块，为1就表示其中一块忙，为0表示两块都闲或都在使用。系统每次分配和回收伙伴块时都要对它们的伙伴位跟1进行[异或运算](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96%E8%BF%90%E7%AE%97)。所谓[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)是指刚开始时，两个伙伴块都空闲，它们的伙伴位为0，如果其中一块被使用，[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得1；如果另一块也被使用，[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得0；如果前面一块回收了[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得1；如果另一块也回收了[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得0。
**位图的主要用途是在回收算法中指示是否可以和伙伴块合并，分配时只要搜索空闲链表就足够了。当然，分配的同时还要对相应位异或一下，这是为回收算法服务**



**记录大小不同的空闲页**

![img](https:////upload-images.jianshu.io/upload_images/5346502-8d12e8a8d7d5d353.png?imageMogr2/auto-orient/strip|imageView2/2/w/550/format/webp)

![img](https:////upload-images.jianshu.io/upload_images/5346502-b76497f021809357.png?imageMogr2/auto-orient/strip|imageView2/2/w/343/format/webp)

**分配和释放过程**

Buddy算法的分配原理：

　　假如系统需要$4(2^2)$个页面大小的内存块，该算法就到free_area[2]中查找，如果链表中有空闲块，就直接从中摘下并分配出去。如果没有，算法将顺着数组向上查找free_area[3],如果free_area[3]中有空闲块，则将其从链表中摘下，分成等大小的两部分，前四个页面作为一个块插入free_area[2]，后4个页面分配出去，free_area[3]中也没有，就再向上查找，如果free_area[4]中有，就将这$16(2^4)$个页面等分成两份，前一半挂如free_area[3]的链表头部，后一半的8个页等分成两等分，前一半挂free_area[2]
的链表中，后一半分配出去。假如free_area[4]也没有，则重复上面的过程，知道到达free_area数组的最后，如果还没有则放弃分配。

![image.png](https://s2.loli.net/2022/10/17/6IsoA937UebM1G8.png)



 

 Buddy算法的释放原理：

　　内存的释放是分配的逆过程，也可以看作是伙伴的合并过程。当释放一个块时，先在其对应的链表中考查是否有伙伴存在，如果没有伙伴块，就直接把要释放的块挂入链表头；如果有，则从链表中摘下伙伴，合并成一个大块，然后继续考察合并后的块在更大一级链表中是否有伙伴存在，直到不能合并或者已经合并到了最大的块($2^9$个页面)。





**伙伴算法是静态分区和动态分区法的折中，比静态分区法灵活，不受分区尺寸及个数的限制；比动态分区法规范，不易出现外部碎片。会产生内部碎片，但比静态分区的小。**



**优缺点**

伙伴算法的一大优势是它能够完全避免外部碎片的产生，申请时，伙伴算法会给程序分配一个较大的内存空间，即保证所有大块内存都能得到满足。很明显分配比需求还大的内存空间，会产生内部碎片。所以伙伴算法虽然能够完全避免外部碎片的产生，但这恰恰是以产生内部碎片为代价的。



**适合大小恰好为 $2^k$  大小的，否则会有比较大的内部碎片。（Linux C++ vector 2 倍扩容机制）**



优点：

* 较好的解决外部碎片问题
* 当需要分配若干个内存页面时，用于DMA的内存页面必须连续，伙伴算法很好的满足了这个要求
* 只要请求的块不超过512个页面(2K)，内核就尽量分配连续的页面
* 针对大内存分配设计

缺点：

* 合并的要求太过严格，只能是满足伙伴关系的块才能合并，比如第1块和第2块就不能合并。
* 碎片问题：一个连续的内存中仅仅一个页面被占用，导致整块内存区都不具备合并的条件
* 浪费问题：伙伴算法只能分配2的幂次方内存区，当需要8K（2页）时，好说，当需要9K时，那就需要分配16K（4页）的内存空间，但是实际只用到9K空间，多余的7K空间就被浪费掉。
* 算法的效率问题： 伙伴算法拆分和合并涉及了比较多的计算还有链表和位图的操作，开销还是比较大的，如果每次 $2^n$ 大小的伙伴块就会合并到  $2^{n+1}$ 的链表队列中，那么 $2^n$ 大小链表中的块就会因为合并操作而减少，但系统随后立即有可能又有对该大小块的需求，为此必须再从 $2^{n+1}$ 大小的链表中拆分，这样的合并又立即拆分的过程是无效率的。

**Linux针对大内存的物理地址分配**，采用伙伴算法，如果是针对小于一个page的内存，频繁的分配和释放，有更加适宜的解决方案，如slab和kmem_cache等



**优化?**

* 解决内部碎片过大问题（eg：申请5页，分配8页，浪费3页）：
	* ucore 在前部留下需要的页数，释放掉尾部各页。每次释放1页，先划分成页块，再逐个释放。
* 解决切分与合并过于频繁的问题：
	* 用得较多的是单个页。位于处理器Cache中页称为热页（hot page），其余页称为冷页（cold page）。处理器对热页的访问速度要快于冷页。可建一个热页队列（per_cpu_page），暂存刚释放的单个物理页，将合并工作向后推迟 Lazy。总是试图从热页队列中分配单个物理页。分配与释放都在热页队列的队头进行。
* 解决内存碎化(有足够多的空闲页，但是没有大页块)问题：
	* 将页块从一个物理位置移动到另一个物理位置，并保持移动前后逻辑地址不变（拷贝页块内容）
	* 逻辑内存管理器。
* 满足大内存的需求：

* 物理内存空间都耗尽的情况：
	* 在任何情况下，都应该预留一部分空闲的物理内存以备急需。定义两条基准线low和high，当空闲内存量小于low时，应立刻开始回收物理内存，直到空闲内存量大于high。

* 回收物理内存：
	* 法一：启动一个守护进程，专门用于回收物理内存。周期性启动，也可被唤醒。
	* 法二：申请者自己去回收内存。实际是由内存分配程序回收。回收的方法很多，如释放缓冲区、页面淘汰等。



#### Slab 算法？

slab是Linux操作系统的一种内存分配机制。其工作是针对一些**经常分配并释放的对象**，如进程描述符等，这些对象的**大小一般比较小**，如果直接采用伙伴系统来进行分配和释放，不仅会造成大量的内碎片，而且处理速度也太慢。而slab分配器是基于对象进行管理的，相同类型的对象归为一类(如进程描述符就是一类)，每当要申请这样一个对象，slab分配器就从一个slab列表中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统，从而避免这些内部碎片。slab分配器并不丢弃已分配的对象，而是释放并把它们保存在内存中。当以后又要请求新的对象时，就可以从内存直接获取而不用重复初始化。 


Linux 的slab 可有三种状态：

* 满的：slab 中的所有对象被标记为使用。
* 空的：slab 中的所有对象被标记为空闲。
* 部分：slab 中的对象有的被标记为使用，有的被标记为空闲。

slab 分配器首先从部分空闲的slab 进行分配。如没有，则从空的slab 进行分配。如没有，则从物理连续页上分配新的slab，并把它赋给一个cache ，然后再从新slab 分配空间。



与传统的内存管理模式相比， slab 缓存分配器提供了很多优点。
　　1、内核通常依赖于对小对象的分配，它们会在系统生命周期内进行无数次分配。
　　2、slab 缓存分配器通过对类似大小的对象进行缓存而提供这种功能，从而避免了常见的碎片问题。
　　3、slab 分配器还支持通用对象的初始化，从而避免了为同一目的而对一个对象重复进行初始化。
　　4、slab 分配器还可以支持硬件缓存对齐和着色，这允许不同缓存中的对象占用相同的缓存行，从而提高缓存的利用率并获得更好的性能。



### 非连续存储管理

#### 谈谈内存分段？

**什么是内存分段**

程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。**



**分段机制下，虚拟地址和物理地址是如何映射的？**

分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

![img](https://img-blog.csdnimg.cn/a9ed979e2ed8414f9828767592aadc21.png)

段选择因子和段内偏移量：

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。
- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

在上面，知道了虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：

![img](https://img-blog.csdnimg.cn/c5e2ab63e6ee4c8db575f3c7c9c85962.png)

如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。



**分段的优点**



**分段的缺点及解决方案**

分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：

- 第一个就是**内存碎片**的问题。
- 第二个就是**内存交换的效率低**的问题。

接下来，说说为什么会有这两个问题。

> 我们先来看看，分段为什么会产生内存碎片的问题？

我们来看看这样一个例子。假设有 1G 的物理内存，用户执行了多个程序，其中：

- 游戏占用了 512MB 内存
- 浏览器占用了 128MB 内存
- 音乐占用了 256 MB 内存。

这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。

如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。

![img](https://img-blog.csdnimg.cn/6142bc3c917e4a6298bdb62936e0d332.png)

> 内存分段会出现内存碎片吗？

内存碎片主要分为，内部内存碎片和外部内存碎片。

内存分段管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以**不会出现内部内存碎片**。

但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以**会出现外部内存碎片**的问题。

解决「外部内存碎片」的问题就是**内存交换**。

可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。

> 再来看看，分段为什么会导致内存交换效率低的问题？

对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。

因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。

所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

为了解决内存分段的「外部内存碎片和内存交换效率低」的问题，就出现了内存分页



#### 谈谈内存分页？

**什么是内存分页**

分段的好处就是能产生连续的内存空间，但是会出现「外部内存碎片和内存交换的空间太大」的问题。

要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

虚拟地址与物理地址之间通过**页表**来映射，如下图：

![img](https://img-blog.csdnimg.cn/08a8e315fedc4a858060db5cb4a654af.png)

页表是存储在内存里的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。

而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。



**分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？**

内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而**采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。**

但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对**内存分页机制会有内部内存碎片**的现象。

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**

![img](https://img-blog.csdnimg.cn/388a29f45fe947e5a49240e4eff13538.png)

更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是**只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。**





**分页机制下，虚拟地址和物理地址是如何映射的？**

在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。

![img](https://img-blog.csdnimg.cn/7884f4d8db4949f7a5bb4bbd0f452609.png)

总结一下，对于一个内存地址转换，其实就是这样三个步骤：

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：

![img](https://img-blog.csdnimg.cn/8f187878c809414ca2486b0b71e8880e.png)

这看起来似乎没什么毛病，但是放到实际中操作系统，这种简单的分页是肯定是会有问题的。



**分页的优点**



**分页的缺点及解决方案**

有空间上的缺陷。

因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。

在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 `4MB` 的内存来存储页表。

这 4MB 大小的页表，看起来也不是很大。但是要知道每个进程都是有自己的虚拟地址空间的，也就说都有自己的页表。

那么，`100` 个进程的话，就需要 `400MB` 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。



**多级页表**

要解决上面的问题，就需要采用一种叫作**多级页表**（*Multi-Level Page Table*）的解决方案。

在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 `4KB` 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。

我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**。如下图所示：

![img](https://img-blog.csdnimg.cn/19296e249b2240c29f9c52be70f611d5.png)

> 你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？

当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。

其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的**局部性原理**么？

每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。

如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= `0.804MB`，这对比单级页表的 `4MB` 是不是一个巨大的节约？

那么为什么不分级的页表就做不到这样节约内存呢？

我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。

我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。

对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%9B%9B%E7%BA%A7%E5%88%86%E9%A1%B5.png)



**TLB**

多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。

![img](https://img-blog.csdnimg.cn/edce58534d9342ff89f5261b1929c754.png)

我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（*Translation Lookaside Buffer*） ，通常称为页表缓存、转址旁路缓存、快表等。

![img](https://img-blog.csdnimg.cn/a3cdf27646b24614a64cfc5d7ccffa35.png)

在 CPU 芯片里面，封装了内存管理单元（*Memory Management Unit*）芯片，它用来完成地址转换和 TLB 的访问与交互。

有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。

TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。



#### 段页式内存管理？

内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为**段页式内存管理**。

![img](https://img-blog.csdnimg.cn/f19ebd6f70f84083b0d87cc5e9dea8e3.png)

段页式内存管理实现的方式：

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；

这样，地址结构就由**段号、段内页号和页内位移**三部分组成。

用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：

![img](https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png)

段页式地址变换中要得到物理地址须经过三次内存访问：

- 第一次访问段表，得到页表起始地址；
- 第二次访问页表，得到物理页号；
- 第三次将物理页号与页内位移组合，得到物理地址。

可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。
