# 内存

## 问题

### 虚拟内存机制

* 为什么要虚拟内存，有什么用？
* 如何实现虚拟内存？
	* 硬件支持
	* 软件支持
* 谈谈请求页式存储管理？
* 谈谈请求段式存储管理？
* Linux 进程的内存分布长什么样？



### malloc 和 free

* malloc() 如何分配内存？

* malloc() 分配的是物理内存吗？

* malloc(1) 会分配多大内存？

* malloc() 为什么不全部使用 mmap 来分配内存？

* malloc() 为什么不全部使用 brk 来分配内存？

* malloc() 如果没有成功，可能什么原因，计算机会做什么？

* malloc() 是线程安全的吗 ?是可重入的吗？

* free() 释放内存，会归还给操作系统吗？

* free() 函数只传入一个内存地址，为什么会知道要释放多大的内存？

	

### 连续存储管理

* 静态分区分配是什么？有什么优缺点？
* 动态分区分配是什么？有什么优缺点？
* 什么是内部碎片？什么是外部碎片？各自有什么解决方法？



* 首次适应算法？
* 最佳适应算法？
* 最差适应算法？
* 邻近适应算法？



* Buddy 伙伴算法
* Slab 算法？

### 非连续存储管理

* 谈谈内存分段？
	* 什么是内存分段
	* 分段机制下，虚拟地址和物理地址是如何映射的？
	* 分段的优点
	* 分段的缺点及解决方案
* 谈谈内存分页？
	* 什么是内存分页
	* 分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？
	* 分页机制下，虚拟地址和物理地址是如何映射的？
	* 分页的优点
	* 分页的缺点及解决方案
	* 多级页表
	* TLB
* 谈谈段页式内存管理



### 页面置换算法

https://taifua.com/ostep-vm-beyondphys-policy.html

* 谈谈系统抖动与工作集
* 谈谈 Belady 异常
* 谈谈最佳页面置换算法 OPT
* 谈谈先进先出置换算法 FIFO
* 谈谈第二次机会页面置换算法 SC
* 谈谈最近最久未使用置换算法 LRU
* 谈谈如何优化传统的 LRU（预读、缓存失效）
* 谈谈时钟页面置换算法 CLOCK（近似 LRU）
* 谈谈最不常用置换算法 LFU/NFU
* 谈谈老化算法
* 谈谈工作集页面置换算法
* 工作集时钟页面置换算法
* 谈谈随机算法
* Linux 内核页面置换算法



TLB 命中和未命中会怎么样

什么时候 flush TLB？如何尽可能避免 TLB？

什么时候会发生 segment fault 数组越界一定会发生吗

mmap 的单位是多少

知道哪些 allocator（TCmalloc）

对于缺页中断这种情况，linux操作系统是如何减少页面的换入换出的呢，有哪些方式







## 回答

### 虚拟内存机制

#### 为什么要虚拟内存，有什么用？

单片机是没有操作系统的，所以每次写完代码，都需要借助工具把程序烧录进去，这样程序才能跑起来。

另外，**单片机的 CPU 是直接操作内存的「物理地址」**。

![img](https://img-blog.csdnimg.cn/019f1f0d2d30469cbda2b8fe2cf5e622.png)

在这种情况下，要想在内存中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃。

> 操作系统是如何解决这个问题呢？

这里关键的问题是这两个程序都引用了绝对物理地址，而这正是我们最需要避免的。

我们可以把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。

![进程的中间层](https://img-blog.csdnimg.cn/img_convert/298fb68e3da94d767b02f2ed81ebf2c4.png)

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。

操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：

![img](https://img-blog.csdnimg.cn/72ab76ba697e470b8ceb14d5fc5688d9.png)

**虚拟内存的作用**

- 第一，虚拟内存可以使得进程的运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。
- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。



#### 谈谈请求页式存储管理？





#### 谈谈请求段式存储管理？



#### Linux 进程的内存分布长什么样？

那么，Linux 操作系统采用了哪种方式来管理内存呢？

> 在回答这个问题前，我们得先看看 Intel 处理器的发展历史。

早期 Intel 的处理器从 80286 开始使用的是段式内存管理。但是很快发现，光有段式内存管理而没有页式内存管理是不够的，这会使它的 X86 系列会失去市场的竞争力。因此，在不久以后的 80386 中就实现了页式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了页式内存管理。

但是这个 80386 的页式内存管理设计时，没有绕开段式内存管理，而是建立在段式内存管理的基础上，这就意味着，**页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。**

由于此时由段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由页式内存管理将线性地址映射成物理地址。

![img](https://img-blog.csdnimg.cn/bc0aaaf379fc4bc8882efd94b9052b64.png)

这里说明下逻辑地址和线性地址：

- 程序所使用的地址，通常是没被段式内存管理映射的地址，称为逻辑地址；
- 通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址；

逻辑地址是「段式内存管理」转换前的地址，线性地址则是「页式内存管理」转换前的地址。

> 了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采用了什么方式管理内存？

**Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制**。

这主要是上面 Intel 处理器发展历史导致的，因为 Intel X86 CPU 一律对程序中使用的地址先进行段式映射，然后才能进行页式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。

但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作用。也就是说，“上有政策，下有对策”，若惹不起就躲着走。

**Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。**

> 我们再来瞧一瞧，Linux 的虚拟地址空间是如何分布的？

在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：

![img](https://img-blog.csdnimg.cn/3a6cb4e3f27241d3b09b4766bb0b1124.png)

通过这里可以看出：

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

再来说说，内核空间与用户空间的区别：

- 进程在用户态时，只能访问用户空间内存；
- 只有进入内核态后，才可以访问内核空间的内存；

虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。

![img](https://img-blog.csdnimg.cn/48403193b7354e618bf336892886bcff.png)

接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。

我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系：

![虚拟内存空间划分](https://img-blog.csdnimg.cn/img_convert/b4f882b9447760ce5321de109276ec23.png)

通过这张图你可以看到，用户空间内存，从**低到高**分别是 6 种不同的内存段：

- 程序文件段（.text），包括二进制可执行代码；
- 已初始化数据段（.data），包括静态常量；
- 未初始化数据段（.bss），包括未初始化的静态变量；
- 堆段，包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关 (opens new window)](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；

上图中的内存布局可以看到，程序文件段（.text）下面还有一段内存空间的（灰色部分），这一块区域是「保留区」，之所以要有保留区这是因为在大多数的系统里，我们认为比较小数值的地址不是一个合法地址，例如，我们通常在 C 的代码里会将无效的指针赋值为 NULL。因此，这里会出现一段不可访问的内存保留区，防止程序因为出现 bug，导致读或写了一些小内存地址的数据，而使得程序跑飞。

在这 7 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。



**源码视角**

https://blog.csdn.net/FF_programming/article/details/120963212

**1、task_struct**

每个进程在内核中都有一个进程控制块(PCB)来维护进程相关的信息，Linux内核的进程控制块是task_struct[结构体](https://so.csdn.net/so/search?q=结构体&spm=1001.2101.3001.7020)。

- 位置：<include\linux\sched.h> - 593行
- 部分代码如下：

```c
struct task_struct
{
    /*...*/
	struct mm_struct		*mm;
	struct mm_struct		*active_mm;

	/* Per-thread vma caching: */
	struct vmacache			vmacache;
    /*...*/
}

```

* `*mm`
mm为mm_struct类型的指针，指向mm_struct结构，对于普通的用户进程来说mm字段指向他的虚拟地址空间的用户空间部分，对于内核线程来说这部分为NULL。
* `*active_mm`
  对于匿名进程，也可以理解为内核线程，它的mm字段为NULL，表示没有内存地址空间，可也并不是真正的没有，这是因为所有进程关于内核的映射都是一样的，内核线程可以使用任意进程的地址空间。
  **内核schedule在进程上下文切换的时候，会根据task->mm判断即将调度的进程是用户进程还是内核线程。**
  由于内核线程之前可能是任何用户层进程在执行，故用户空间部分的内容本质上是随机的，内核线程决不能修改其内容，故将mm设置为NULL。所以对于内核线程来说task_struct->mm == NULL，而task_struct->active_mm为某个进程的mm。
  如果切换出去的是用户进程，内核将原来进程的mm存放在新内核线程的active_mm中，因为某些时候内核必须知道用户空间当前包含了什么。
  而当内核线程离开的时候，会把借用的地址空间将会返还给原来的进程，并且清除这一字段。
* `vmcacahe`
  vmacache结构体定义在<include\linux\mm_types_task.h>的34行，如下：

```c
#define VMACACHE_BITS 2
#define VMACACHE_SIZE (1U << VMACACHE_BITS)

struct vmacache {
  u64 seqnum;
  struct vm_area_struct *vmas[VMACACHE_SIZE];
};

```

可以看到它是一个vm_area_struct类型的数组指针，表示指向几段vma。
这是由于为了提高vma的查找速度，由传统的链表式改为了红黑树管理，然而某些进程地址空间中的vma数量众多，而查找vma又是内核中非常频繁的操作，所以为了进一步加快查找速度，内核采取了一种缓存方式，就是把最近访问的几个vma保存起来。
VMACACHE_SIZE为左移2位的一个无符号整形数字，这里感觉大概是保存了4个最近访问的vma。



**2、mm_struct**

每一个进程都会有唯一的mm_struct结构体

- 位置：<include\linux\mm_types.h> - 340行
- 部分代码如下：

```c
struct mm_struct
{
    /*...*/
    struct vm_area_struct *mmap;		/* list of VMAs */
	struct rb_root mm_rb;
	u64 vmacache_seqnum;                /* per-thread vmacache */
    unsigned long mmap_base;	/*映射基地址*/
	unsigned long mmap_legacy_base;	/*不是很明白这里*/
    unsigned long task_size;	/*该进程能够vma使用空间大小*/
	unsigned long highest_vm_end;	/*该进程能够使用的vma结束地址*/
	pgd_t * pgd;
    atomic_t mm_users;
    atomic_t mm_count;
    int map_count;			/* vma的总个数 */
    unsigned long total_vm;	   /* 映射的总页面数*/
    /*...*/
    unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;
    /*...*/

```

* *mmap
	该进程内已经使用的进程虚拟空间vm_are_struct结构，以双链表形式存储该指针指向vma链表的头节点。
* mm_rb
	红黑树组织形式，用于快速查找。
	rb_boot结构体定义在<include\linux\rbtree.h>的43行

```c
struct rb_root {
  struct rb_node *rb_node;
};
```


rb_node结构体定义在<include\linux\rbtree.h>的36行

```c
struct rb_node {
  unsigned long  __rb_parent_color;
  struct rb_node *rb_right;
  struct rb_node *rb_left;
  } __attribute__((aligned(sizeof(long))));
```


可以看到该数据结构包含了父节点颜色、左子树指针、右子树指针等字段

* vmacache_seqnum
	vma的缓存策略所使用的字段，表示每个进程都有的几个缓存vma，对应task_struct->vmacache->seqnum。
* *pgd
	**该进程页目录表，把新进程mm_struct的PGD字段填充到CR3中就完成了页表的切换。**
* mm_users、mm_count
	为了支持mm与active_mm，在这里设置了两个计数器，mm_users主要用来记录共用此命名空间的线程数量，mm_count则主要记录对此mm_struct结构体的引用情况。
	也就是说mm_users表示有多少真正使用地址空间的进程，当copy_mm函数在创建进程的时候，遇到CLONE_VM字段的话就再到需要让父子进程共享同一个地址空间，这时会把mm_users加1。而mm_count则表示有多少内核线程引用了这个地址空间。
	当mm_users为0时就会释放mm_count，当mm_count为0时结构体mm_struct将会被释放。



**3、vm_area_struct**
内核每次为用户空间中分配一个空间使用时，都会生成一个vm_area_struct结构用于记录跟踪分配情况，一个vm_area_struct就代表一段虚拟内存空间。

位置：<include\linux\mm_types.h> - 264行
全部代码如下：

```c
struct vm_area_struct {
	unsigned long vm_start; //虚存区起始
	unsigned long vm_end;   //虚存区结束
	struct vm_area_struct *vm_next, *vm_prev;   //前后指针
	struct rb_node vm_rb;   //红黑树中的位置
	unsigned long rb_subtree_gap;
	struct mm_struct *vm_mm;    //所属的 mm_struct
	pgprot_t vm_page_prot;      
	unsigned long vm_flags;     //标志位
	struct {
		struct rb_node rb;
		unsigned long rb_subtree_last;
	} shared;	
	struct list_head anon_vma_chain;
	struct anon_vma *anon_vma;
	const struct vm_operations_struct *vm_ops;  //vma对应的实际操作
	unsigned long vm_pgoff;     //文件映射偏移量
	struct file * vm_file;      //映射的文件
	void * vm_private_data;     //私有数据
	atomic_long_t swap_readahead_info;
#ifndef CONFIG_MMU
	struct vm_region *vm_region;	/* NOMMU mapping region */
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
#endif
	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
} __randomize_layout;

```

***vm_ops**
**vm_operations_struct**结构体定义在<include\linux\mm.h>的393行

```c
struct vm_operations_struct {
  void (*open)(struct vm_area_struct * area);
  void (*close)(struct vm_area_struct * area);
  int (*split)(struct vm_area_struct * area, unsigned long addr);
  int (*mremap)(struct vm_area_struct * area);
  vm_fault_t (*fault)(struct vm_fault *vmf);
  vm_fault_t (*huge_fault)(struct vm_fault *vmf,
  		enum page_entry_size pe_size);
  void (*map_pages)(struct vm_fault *vmf,
  		pgoff_t start_pgoff, pgoff_t end_pgoff);
  unsigned long (*pagesize)(struct vm_area_struct * area);

  /* notification that a previously read-only page is about to become
   * writable, if an error is returned it will cause a SIGBUS */
  vm_fault_t (*page_mkwrite)(struct vm_fault *vmf);

  /* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */
  vm_fault_t (*pfn_mkwrite)(struct vm_fault *vmf);

  /* called by access_process_vm when get_user_pages() fails, typically
   * for use by special VMAs that can switch between memory and hardware
   */
  int (*access)(struct vm_area_struct *vma, unsigned long addr,
  	      void *buf, int len, int write);

  /* Called by the /proc/PID/maps code to ask the vma whether it
   * has a special name.  Returning non-NULL will also cause this
   * vma to be dumped unconditionally. */
  const char *(*name)(struct vm_area_struct *vma);

  #ifdef CONFIG_NUMA
  /*
   * set_policy() op must add a reference to any non-NULL @new mempolicy
   * to hold the policy upon return.  Caller should pass NULL @new to
   * remove a policy and fall back to surrounding context--i.e. do not
   * install a MPOL_DEFAULT policy, nor the task or system default
   * mempolicy.
   */
  int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);

  /*
   * get_policy() op must add reference [mpol_get()] to any policy at
   * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
   * in mm/mempolicy.c will do this automatically.
   * get_policy() must NOT add a ref if the policy at (vma,addr) is not
   * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.
   * If no [shared/vma] mempolicy exists at the addr, get_policy() op
   * must return NULL--i.e., do not "fallback" to task or system default
   * policy.
   */
  struct mempolicy *(*get_policy)(struct vm_area_struct *vma,
  				unsigned long addr);
  #endif
  /*
   * Called by vm_normal_page() for special PTEs to find the
   * page for @addr.  This is useful if the default behavior
   * (using pte_page()) would not find the correct page.
   */
  struct page *(*find_special_page)(struct vm_area_struct *vma,
  				  unsigned long addr);
  };


```

vm_operations结构中包含的是函数指针，其中open为虚存区的打开，close用于虚存区的关闭，另外还有split、mremap、acess等操作。

shared
对于具有地址空间和后备存储的区域，链接到地址空间的i_mmap间隔树。

rb_subtree_gap
该vma子树最大可用内存间隔（以字节为单位），具体为与vma和vma->vm_prev之间，或子vma和它的vma->vm_prev之间的内存间隙，有助于找到大小合适的空闲区域。

vm_flags
描述该区域的一段标志，包括其读写属性等，采用bit位方式，状态标志位定义在<include\linux\mm.h>文件173行中：

```c
  #define VM_NONE		0x00000000  //无标志

  #define VM_READ		0x00000001	//可读
  #define VM_WRITE	0x00000002  //可写
  #define VM_EXEC		0x00000004  //可操作
  #define VM_SHARED	0x00000008  //允许被多个线程访问

  /* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */
  #define VM_MAYREAD	0x00000010	//允许设置可读
  #define VM_MAYWRITE	0x00000020	//允许设置可写  
  #define VM_MAYEXEC	0x00000040	//允许设置可操作
  #define VM_MAYSHARE	0x00000080	//允许设置可共享

  #define VM_GROWSDOWN	0x00000100	//向低地址增长
  #define VM_UFFD_MISSING	0x00000200	
  #define VM_PFNMAP	0x00000400	
  #define VM_DENYWRITE	0x00000800	//不允许写入
  #define VM_UFFD_WP	0x00001000	
  #define VM_LOCKED	0x00002000	//VMA被锁定，不会被交换到交换分区
  #define VM_IO           0x00004000	//该VMA用于IO 映射

  #define VM_SEQ_READ	0x0000800	//应用程序会顺序读该VMA的内容
  #define VM_RAND_READ	0x00010000	//应用程序会随机读取该VMA内存
  #define VM_DONTCOPY	0x00020000	//fork时不会复制该VMA
  #define VM_DONTEXPAND	0x00040000	
  #define VM_LOCKONFAULT	0x00080000	//当处于page fault时锁定该VMA 对应的物理页
  #define VM_ACCOUNT	0x00100000	
  #define VM_NORESERVE	0x00200000	//不做保留
  #define VM_HUGETLB	0x00400000	//huge TLB page对应的VM
  #define VM_SYNC		0x00800000	//page fault时同步映射
  #define VM_ARCH_1	0x01000000	
  #define VM_WIPEONFORK	0x02000000	//不会从父进程相应的VMA中复制页表到子进程的VMA中
  #define VM_DONTDUMP	0x04000000	//VMA不会被包含到core dump中

  #ifdef CONFIG_MEM_SOFT_DIRTY
  # define VM_SOFTDIRTY	0x08000000	
  #else
  # define VM_SOFTDIRTY	0
  #endif

  #define VM_MIXEDMAP	0x10000000	
  #define VM_HUGEPAGE	0x20000000	
  #define VM_NOHUGEPAGE	0x40000000	
  #define VM_MERGEABLE	0x80000000	

```

- **vm_page_prot**
	vm_flags代表VMA的状态位，但是相应的状态位最后要转化成实际内存的下发到硬件状态位，而pgprot则是代表实际物理页状态位。



**4、关系图**![在这里插入图片描述](https://img-blog.csdnimg.cn/703ddd089fd643f481d7d41c2a91531e.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBARkZfcHJvZ3JhbW1pbmc=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

CSAPP 虚拟内存章节看一下

linux内核使用vm_area_struct结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制都不同，因此一个进程使用多个vm_area_struct结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct结构使用链表或者树形结构链接，方便进程快速访问，如下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/img_convert/33f6c2014042fc8356990f33df47a22f.png)

### malloc 和 free

#### malloc() 如何分配内存？

实际上，malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。

- 方式一：通过 brk() 系统调用从堆分配内存
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；

方式一实现的方式很简单，就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/brk%E7%94%B3%E8%AF%B7.png)

方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/mmap%E7%94%B3%E8%AF%B7.png)

> 什么场景下 malloc() 会通过 brk() 分配内存？又是什么场景下通过 mmap() 分配内存？

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
- 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

注意，不同的 glibc 版本定义的阈值也是不同的。



man 手册

> 通常，malloc () 从堆中分配内存，并调整使用sbrk(2)根据需要设置堆的大小。当分配
> 大于MMAP_THRESHOLD字节的内存块时，glibc malloc () 实现使用mmap(2)将内存分配为私有匿名映射。  MMAP_THRESHOLD默认为 128 kB ，但可以使用mallopt(3)进行调整。在 Linux 4.7 之前，使用 mmap(2)执行的分配不受 RLIMIT_DATA资源限制的影响；自 Linux 4.7 起，此限制也适用于使用 mmap(2) 执行的分配。



#### malloc() 分配的是物理内存吗？

不是的，**malloc() 分配的是虚拟内存**。

如果分配后的虚拟内存没有被访问的话，虚拟内存是不会映射到物理内存的，这样就不会占用物理内存了。

只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。



#### malloc(1) 会分配多大内存？

malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。

具体会预分配多大的空间，跟 malloc 使用的内存管理器有关系，我们就以 malloc 默认的内存管理器（Ptmalloc2）来分析。

接下里，我们做个实验，用下面这个代码，通过 malloc 申请 1 字节的内存时，看看操作系统实际分配了多大的内存空间。

```c
#include <stdio.h>
#include <malloc.h>

int main() {
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
  
  //申请1字节的内存
  void *addr = malloc(1);
  printf("此1字节的内存起始地址：%x\n", addr);
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
 
  //将程序阻塞，当输入任意字符时才往下执行
  getchar();

  //释放内存
  free(addr);
  printf("释放了1字节的内存，但heap堆并不会释放\n");
  
  getchar();
  return 0;
}
```

执行代码（**先提前说明，我使用的 glibc 库的版本是 2.17**）：

![图片](https://img-blog.csdnimg.cn/img_convert/080ee187c8c92db45092b6688774e8da.png)

我们可以通过 /proc//maps 文件查看进程的内存分布情况。我在 maps 文件通过此 1 字节的内存起始地址过滤出了内存地址的范围。

```shell
[root@xiaolin ~]# cat /proc/3191/maps | grep d730
00d73000-00d94000 rw-p 00000000 00:00 0                                  [heap]
```

这个例子分配的内存小于 128 KB，所以是通过 brk() 系统调用向堆空间申请的内存，因此可以看到最右边有 [heap] 的标识。

可以看到，堆空间的内存地址范围是 00d73000-00d94000，这个范围大小是 132KB，也就说明了 **malloc(1) 实际上预分配 132K 字节的内存**。

可能有的同学注意到了，程序里打印的内存起始地址是 `d73010`，而 maps 文件显示堆内存空间的起始地址是 `d73000`，为什么会多出来 `0x10` （16字节）呢？这个问题，我们先放着，后面会说。



#### malloc() 为什么不全部使用 mmap 来分配内存？

因为向操作系统申请内存，是要通过系统调用的，执行系统调用是要进入内核态的，然后在回到用户态，运行态的切换会耗费不少时间。

所以，申请内存的操作应该避免频繁的系统调用，如果都用 mmap 来分配内存，等于每次都要执行系统调用。

另外，因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断。

也就是说，**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**。

为了改进这两个问题，malloc 通过 brk() 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中。

**等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗**。



#### mmap 实现原理？



#### malloc() 为什么不全部使用 brk 来分配内存？

前面我们提到通过 brk 从堆空间分配的内存，并不会归还给操作系统，那么我们那考虑这样一个场景。

如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。

![图片](https://img-blog.csdnimg.cn/img_convert/75edee0cb75450e7987a8a482b975bda.png)

但是如果下次申请的内存大于 30k，没有可用的空闲内存空间，必须向 OS 申请，实际使用内存继续增大。

因此，随着系统频繁地 malloc 和 free ，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的。

所以，malloc 实现中，充分考虑了 brk 和 mmap 行为上的差异及优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间。



#### malloc() 如果没有成功，可能什么原因，计算机会做什么？

*  内存不足。 
*  在前面的程序中出现了内存的越界访问，导致malloc()分配函数所涉及的一些信息被破坏。下次再使用malloc()函数申请内存就会失败，返回空指针NULL(0)。

查看方式：

1、内存不足，使用free命令查看当前还有多少内存，看是否合理，之前是否有内存泄漏等。

2、按照流程查看malloc失败前的几次malloc、memcpy或字符串拷贝等，查看是否有内存越界。



man 手册

>  默认情况下，Linux 遵循乐观的内存分配策略。这意味着当malloc () 返回非 NULL 时，不能保证内存确实可用。如果 发现系统内存不足，OOM 杀手将杀死一个或多个进程。更多信息请参见proc(5)中/proc/sys/vm/overcommit_memory和 /proc/sys/vm/oom_adj的描述，以及 Linux 内核源文件 Documentation/vm/overcommit-accounting.rst。



#### malloc() 是线程安全的吗 ?是可重入的吗？

**线程安全**

man 手册

> 为了避免多线程应用程序中的损坏，内部使用互斥锁来保护这些函数使用的内存管理数据结构。在线程同时分配和释放内存的多线程应用程序中，可能存在对这些互斥体的争用。为了在多线程应用程序中可伸缩地处理内存分配，如果检测到互斥争用，glibc 会创建
> 额外的内存分配区域。每个 arena 都是由系统内部分配的大内存区域（使用brk(2)或mmap(2)），并且使用自己的互斥锁进行管理。



https://blog.csdn.net/qq_40334837/article/details/96423711

**malloc 可重入吗？**

在man手册中，与系统调用有关的函数都会说明该函数是否线程安全，所以这也是我们写代码需要关注的，而线程安全与函数是否可重入有很大关系，**函数可重入一定是线程安全的，线程安全不一定是可重入函数，比如maloc使用递归锁实现了线程安全，但它是不可重入函数**，所以不可重入函数可以通过内核锁实现线程安全（锁是系统调用，所以工作在内核态，也叫内核锁），还有很多函数也是这样实现线程安全的



**信号**

1、基本概念

软中断信号（signal，又简称为信号）用来**通知进程发生了异步事件**。进程之间可以互相通过系统调用kill发送软中断信号。内核也可以因为内部事件而给进程发送信号，通知进程发生了某个事件。注意，信号只是用来通知某进程发生了什么事件，并不给该进程传递任何数据。

收到信号的进程对各种信号有不同的处理方法。处理方法可以分为三类：

* 第一种是类似中断的处理程序，对于需要处理的信号，进程可以指定处理函数，由该函数来处理。
* 第二种方法是，忽略某个信号，对该信号不做任何处理，就象未发生过一样。
* 第三种方法是，对该信号的处理保留系统的默认值，这种缺省操作，对大部分的信号的缺省操作是使得进程终止。进程通过系统调用signal来指定进程对某个信号的处理行为。



**内核对信号的基本处理方法**

**内核处理一个进程收到的信号的时机是在一个进程从内核态返回用户态时。所以，当一个进程在内核态下运行时，软中断信号并不立即起作用，要等到将返回用户态时才处理**，比如线程中系统调用malloc申请内存，所以从用户态进入了内核态，现在信号发生，这时操作系统会从内核态跳转到用户态执行signal函数（signal函数本身是应用层的一行代码需要被运行，属于用户态，当执行到绑定的函数时，函数内部有系统调用函数，然后进入内核态）。进程收到一个要捕捉的信号，那么进程从内核态返回用户态时执行用户定义的函数，而且执行用户定义的函数的方法很巧妙（这个函数时signal函数绑定的那个处理信号函数，比如里面出现malloc系统调用），内核是在用户栈上创建一个新的层，该层中将返回地址的值设置成用户定义的处理函数的地址，这样进程从内核返回弹出栈顶时就返回到用户定义的函数处，从函数返回再弹出栈顶时，才返回原先进入内核的地方（线程中调用malloc所在内核执行处）。这样做的原因是用户定义的处理函数不应该在内核态下执行，所以一般在信号处理函数中，最好仅仅用来打印一条信息，然后使用longjmp或者exit退出。

![img](https://img-blog.csdnimg.cn/20190718103716258.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzM0ODM3,size_16,color_FFFFFF,t_70)

综上所述，如果信号被捕获，执行点从内核态返回用户态，在返回时，如果发现待执行进程存在被触发的signal，那么在离开内核态之后（也就是将CPU切换到用户模式），执行用户进程为该signal绑定的signal处理函数，从这一点上看，signal处理函数是在用户进程上下文中执行的。当执行完signal处理函数之后，再返回到用户进程被中断或者system call（软中断或者指令陷阱）打断的地方。



**如果signal处理函数使用系统调用，比如malloc，free**

信号处理函数中只能调用可重入函数，而不能调用不可重入函数。进程捕捉到信号并对其进行处理时，正在执行的正常指令序列就被信号处理程序临时中断，它首先执行该信号处理函数中的指令。如果从信号处理程序返回，则继续执行在捕捉到信号时正在执行的正常指令序列（这类似于发生硬件中断时所做的）。但在信号处理函数中，不能判断捕捉到信号时线程执行到何处。

 信号处理函数默认情况下是在进程的主线程调用的，这种情况下使用不可重入函数，有可能会造成不可预知的错误。比如调用了malloc函数，为了保证malloc是线程安全的，所以内部使用了锁，根据malloc中锁的不同处理方式，分别可能会导致以下情况的发生：

1)  如果是普通锁，在主线程中malloc函数获取锁之后被signal中断，在signal处理函数中继续调用malloc，因为主线程中的malloc已经获取到了锁，signal处理函数只能等待锁释放，而主线程中的malloc函数正在等待signal处理函数返回后继续执行，这样就造成了锁死；

2)  如果是递归锁，那么signal处理函数中的malloc函数获取锁后进行内存分配，因为上次的malloc操作还没完，可能成会造成内存数据混乱。




就定时而言，可不直接使用singal alarm，而使用posix定时器，通过通知线程的方式，将定时处理函数放到单独的线程中来处理。



**内核线程调度系统对malloc的处理方法**

多线程之前使用malloc是安全的，虽然它不可重入，但是用锁实现了

1)  如果是普通锁，A线程获取堆栈锁后，B线程必须等待A线程执行完成后，释放锁，然后B线程malloc才能申请内存。

2)  如果是递归锁，内核调度系统在线程之间调度时，如果线程A的malloc函数一旦开始了申请，它就不会交出CPU，而是等malloc完成后，才会根据情况是否交出CPU，如果没有特别重要的处理，调度器就会跳转到线程B中执行，如果B线程执行到malloc，同理。（调度器如何工作等我搞明白再修正这




**代码例子**

https://zhuanlan.zhihu.com/p/371222679

```c
#include <stdio.h>
#include <unistd.h>
#include <signal.h>
#include <string.h>
#include <stdlib.h>

void sig_handler(int signum)
{
    printf("\nInside handler function\n");
    char *p = NULL;
    p = (char *)malloc(sizeof(char) * 1024);
    if (NULL == p)
    {
        return;
    }
    free(p);
}


int main(int argc, char **argv)
{
    char *p = NULL;
    signal(SIGINT, sig_handler); // 初始化信号处理函数
    
    while (1)
    {
        p = (char *)malloc(sizeof(char) * 1024);
        if (NULL == p)
        {
            continue;
        }
        free(p);
    }

    return 0;
}
```

编译上述代码：

```text
gcc -g testsigint.c -o testsigint
```

运行代码，不停的按ctrl+c（要亿点点运气）

```text
[root@root]# ./testsigint
^C
Inside handler function
^C
Inside handler function
^C
Inside handler function
^C
Inside handler function
^C^C^C^C^C^C
```

当出现多个^C的时候，证明该进程已经锁死了。

新开一个窗口，获取该进程的堆栈打印。

```text
[root@root]# pstack `pidof testsigint`
#0  0x00007faae50275cc in __lll_lock_wait_private () from /lib64/libc.so.6
#1  0x00007faae4fa3b12 in _L_lock_16654 () from /lib64/libc.so.6
#2  0x00007faae4fa0753 in malloc () from /lib64/libc.so.6
#3  0x0000000000400634 in sig_handler (signum=2) at testsigint.c:11
#4  <signal handler called>
#5  0x00007faae4f9bf1b in _int_free () from /lib64/libc.so.6
#6  0x0000000000400699 in main (argc=1, argv=0x7fff29e79aa8) at testsigint.c:32
[root@localhost ~]# pstack `pidof testsigint`
#0  0x00007faae50275cc in __lll_lock_wait_private () from /lib64/libc.so.6
#1  0x00007faae4fa3b12 in _L_lock_16654 () from /lib64/libc.so.6
#2  0x00007faae4fa0753 in malloc () from /lib64/libc.so.6
#3  0x0000000000400634 in sig_handler (signum=2) at testsigint.c:11
#4  <signal handler called>
#5  0x00007faae4f9bf1b in _int_free () from /lib64/libc.so.6
#6  0x0000000000400699 in main (argc=1, argv=0x7fff29e79aa8) at testsigint.c:32
```

可以看到进程挂死在了malloc和free结对的地方，肯定是有内部资源互斥了。

在进程通过信号产生软中断进入用户自定义的信号处理函数，内核会保存和恢复进程的上下文，然而恢复的上下文仅限于返回地址，cpu寄存器等之类的少量上下文，而用户态函数内部使用的一些全局变量、静态变量，锁等并不在保护之列，所以如果这些值在信号处理函数发生了改变，那么当函数回到用户原本的上下文继续执行时，其结果就不可预料了。

比如上面的malloc，将如一个进程此时正在执行malloc分配堆空间，此时程序捕捉到信号发生中断，执行信号处理程序中恰好也有一个malloc，这样就会对进程的环境造成破坏，因为malloc通常为它所分配的存储区维护一个链接表和一些锁，插入执行信号处理函数时，进程可能正在对这张表进行操作，而信号处理函数的调用可能回去获取同一把锁，也可能回去修改内存里面的值，这样造成的结果是不可预期的。







#### free() 释放内存，会立即归还给操作系统吗？

我们在上面的进程往下执行，看看通过 free() 函数释放内存后，堆内存还在吗？

![图片](https://img-blog.csdnimg.cn/img_convert/1a9337f8f6b83fbc186f257511b5ce67.png)

从下图可以看到，通过 free 释放内存后，堆内存还是存在的，并没有归还给操作系统。

![图片](https://img-blog.csdnimg.cn/img_convert/2b8f63892830553ec04c5f05f336ae8b.png)

这是因为与其把这 1 字节释放给操作系统，不如先缓存着放进 malloc 的内存池里，当进程再次申请 1 字节的内存时就可以直接复用，这样速度快了很多。

当然，当进程退出后，操作系统就会回收进程的所有资源。

上面说的 free 内存后堆内存还存在，是针对 malloc 通过 brk() 方式申请的内存的情况。

如果 malloc 通过 mmap 方式申请的内存，free 释放内存后就会归归还给操作系统。

我们做个实验验证下， 通过 malloc 申请 128 KB 字节的内存，来使得 malloc 通过 mmap 方式来分配内存。

```c
#include <stdio.h>
#include <malloc.h>

int main() {
  //申请1字节的内存
  void *addr = malloc(128*1024);
  printf("此128KB字节的内存起始地址：%x\n", addr);
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());

  //将程序阻塞，当输入任意字符时才往下执行
  getchar();

  //释放内存
  free(addr);
  printf("释放了128KB字节的内存，内存也归还给了操作系统\n");

  getchar();
  return 0;
}
```

执行代码：

![图片](https://img-blog.csdnimg.cn/img_convert/500fdc021d956f60963f308760f511d0.png)

查看进程的内存的分布情况，可以发现最右边没有 [head] 标志，说明是通过 mmap 以匿名映射的方式从文件映射区分配的匿名内存。

![图片](https://img-blog.csdnimg.cn/img_convert/501f458b8d35abe5e378a0f14c667797.png)

然后我们释放掉这个内存看看：

![图片](https://img-blog.csdnimg.cn/img_convert/fcdbe91cc03b6a2f6e93dd1971d1b438.png)

再次查看该 128 KB 内存的起始地址，可以发现已经不存在了，说明归还给了操作系统。

![图片](https://img-blog.csdnimg.cn/img_convert/3f63c56b131d92806b5aabca29d33a38.png)

对于 「malloc 申请的内存，free 释放内存会归还给操作系统吗？」这个问题，我们可以做个总结了：

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。

#### free() 函数只传入一个内存地址，为什么会知道要释放多大的内存？

还记得，我前面提到， malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节吗？

这个多出来的 16 字节就是保存了该内存块的描述信息，比如有该内存块的大小。

![图片](https://img-blog.csdnimg.cn/img_convert/cb6e3ce4532ff0a6bfd60fe3e52a806e.png)

这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。



### 连续存储管理

#### 静态分区分配是什么？有什么优缺点？

优点：简单，要求的硬件支持少，软件算法也简单

缺点：

* 内存利用率低，产生内部碎片
* 尺寸和分区数量难以确定
* 分区总数固定，限制了并发执行的程序数量



#### 动态分区分配是什么？有什么优缺点？

不预先划分内存，在程序装入内存时，根据进程的大小动态地建立分区，并使得分区的大小正好适合进程的需要，因此系统中分区的大小和数目是可变的。

在进程装入或换入主存时，如果内存中有多个足够大的空闲块，操作系统必须确定分配哪个内存块给进程使用，这就是动态分区的分配策略



优点：

*  没有内部碎片
*  能较有效利用内存空间，提高了多道程序系统对内存的共享

缺点：

* 容易产生外部碎片问题，为解决外部碎片问题，需要采用动态重定位，增加了计算机硬件成本，而紧凑工作又要花费大量处理机时间



#### 什么是内部碎片？什么是外部碎片？有什么解决方法？

**内部碎片就是已经被分配出去（能明确指出属于哪个进程）却不能被利用的内存空间；**

内部碎片是处于区域内部或页面内部的存储块。占有这些区域或页面的进程并不使用这个存储块。而在进程占有这块存储块时，系统无法利用它。直到进程释放它，或进程结束时，系统才有可能利用这个存储块。



**外部碎片指的是还没有被分配出去（不属于任何进程），但由于太小了无法分配给申请内存空间的新进程的内存空闲区域。**

外部碎片是出于任何已分配区域或页面外部的空闲存储块。这些存储块的总和可以满足当前申请的长度要求，但是由于它们的地址不连续或其他原因，使得系统无法满足当前申请



**解决外部碎片 --> 紧缩：**

移动内存中的进程，将碎片集中起来，重新构成大的内存块。需要运行时的动态重定位，费时。

紧缩方向：

* 向一头紧缩
* 向两头紧缩。

紧缩时机：

* 在释放分区时，如果不能与空闲分区合并，则立刻进行紧缩。**好处是不存在外部碎片，坏处是费时。**
* 在内存分配时，如果剩余的空闲空间总量能满足要求但没有一个独立的空闲块能满足要求，则进行紧缩。**好处是减少紧缩次数**。





#### 首次适应算法？

思想：从头到尾寻找合适的分区



将所有空闲分区**按照地址递增的次序链接**，在申请内存分配时，从链首开始查找，**将满足需求的第一个**空闲分区分配给作业



优点：

* 综合看，首次适应算法性能最好。**算法开销小**，回收分区后，一般不需要对空闲分区队列重新排序
* 分区集中在内存的前部，大内存留在后面，便于释放后的合并



#### 最佳适应算法？

思想：优先使用更小的分区，以保留更多的大分区



将所有空闲分区按照**从小到大**的顺序形成空闲分区链，在申请内存分配时，总是把**满足需求的、最小的**空闲分区分配给作业



优点：会有更多的大分区被保留下来，更能满足大进程需求

缺点

* 会产生很多太小的、难以利用的外部碎片，需要频繁紧缩
* **算法开销大**，回收分区后可能需要对空闲分区队列重新排序



#### 最差适应算法？

思想：优先使用更大的分区，以防止产生太小的不可用碎片



将所有的空闲分区按照**从大到小**的顺序形成空闲分区链，在申请内存分配时，总是把**满足需求的、最大的**空闲分区分配给作业



优点：优先使用更大的分区，以防止产生太小的不可用碎片

缺点

* 大分区容易被用完，不利于大进程
* **算法开销大**，回收分区后可能需要对空闲分区队列重新排序



#### 邻近适应算法？（循环首次适应算法）

思想：由首次适应算法演变而来，每次从上次查找结束的位置开始查找



将所有空闲分区**按照地址递增的次序链接**（可排列成循环链表），在申请内存分配时，**总是从上次找到的空闲分区的下一个空闲分区开始查找**，将满足需求的第一个空闲分区分配给作业



优点：不用每次都从低地址的小分区开始检索**算法开销小**（原因同首次适应算法）

缺点：会使高地址的大分区也被用完





#### Buddy 伙伴算法？

伙伴算法：将动态分区的大小限定为 2^k  字节，分割方式限定为平分，分区就会变得较为规整，分割与合并会更容易，可以减少一些外部碎片。平分后的两块互称伙伴。

![img](https:////upload-images.jianshu.io/upload_images/5346502-c766de1d4facb6ee.png?imageMogr2/auto-orient/strip|imageView2/2/w/653/format/webp)

分配时可能要多次平分，释放时可能要多次合并。举例：

![img](https:////upload-images.jianshu.io/upload_images/5346502-937492787301e74a.png?imageMogr2/auto-orient/strip|imageView2/2/w/565/format/webp)

**伙伴的概念**，满足以下三个条件的称为伙伴：

* 两个块大小相同；
* 两个块地址连续；
* 两个块必须是同一个大块中分离出来的；



**关于位图**
Linux内核伙伴算法中每个order 的位图都表示所有的空闲块，位图的某位对应于两个伙伴块，为1就表示其中一块忙，为0表示两块都闲或都在使用。系统每次分配和回收伙伴块时都要对它们的伙伴位跟1进行[异或运算](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96%E8%BF%90%E7%AE%97)。所谓[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)是指刚开始时，两个伙伴块都空闲，它们的伙伴位为0，如果其中一块被使用，[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得1；如果另一块也被使用，[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得0；如果前面一块回收了[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得1；如果另一块也回收了[异或](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%BC%82%E6%88%96)后得0。
**位图的主要用途是在回收算法中指示是否可以和伙伴块合并，分配时只要搜索空闲链表就足够了。当然，分配的同时还要对相应位异或一下，这是为回收算法服务**



**记录大小不同的空闲页**

![img](https:////upload-images.jianshu.io/upload_images/5346502-8d12e8a8d7d5d353.png?imageMogr2/auto-orient/strip|imageView2/2/w/550/format/webp)

![img](https:////upload-images.jianshu.io/upload_images/5346502-b76497f021809357.png?imageMogr2/auto-orient/strip|imageView2/2/w/343/format/webp)

**分配和释放过程**

Buddy算法的分配原理：

　　假如系统需要$4(2^2)$个页面大小的内存块，该算法就到free_area[2]中查找，如果链表中有空闲块，就直接从中摘下并分配出去。如果没有，算法将顺着数组向上查找free_area[3],如果free_area[3]中有空闲块，则将其从链表中摘下，分成等大小的两部分，前四个页面作为一个块插入free_area[2]，后4个页面分配出去，free_area[3]中也没有，就再向上查找，如果free_area[4]中有，就将这$16(2^4)$个页面等分成两份，前一半挂如free_area[3]的链表头部，后一半的8个页等分成两等分，前一半挂free_area[2]
的链表中，后一半分配出去。假如free_area[4]也没有，则重复上面的过程，知道到达free_area数组的最后，如果还没有则放弃分配。

![image.png](https://s2.loli.net/2022/10/17/6IsoA937UebM1G8.png)



 

 Buddy算法的释放原理：

　　内存的释放是分配的逆过程，也可以看作是伙伴的合并过程。当释放一个块时，先在其对应的链表中考查是否有伙伴存在，如果没有伙伴块，就直接把要释放的块挂入链表头；如果有，则从链表中摘下伙伴，合并成一个大块，然后继续考察合并后的块在更大一级链表中是否有伙伴存在，直到不能合并或者已经合并到了最大的块($2^9$个页面)。





**伙伴算法是静态分区和动态分区法的折中，比静态分区法灵活，不受分区尺寸及个数的限制；比动态分区法规范，不易出现外部碎片。会产生内部碎片，但比静态分区的小。**



**优缺点**

伙伴算法的一大优势是它能够完全避免外部碎片的产生，申请时，伙伴算法会给程序分配一个较大的内存空间，即保证所有大块内存都能得到满足。很明显分配比需求还大的内存空间，会产生内部碎片。所以伙伴算法虽然能够完全避免外部碎片的产生，但这恰恰是以产生内部碎片为代价的。



**适合大小恰好为 $2^k$  大小的，否则会有比较大的内部碎片。（Linux C++ vector 2 倍扩容机制）**



优点：

* 较好的解决外部碎片问题
* 当需要分配若干个内存页面时，用于DMA的内存页面必须连续，伙伴算法很好的满足了这个要求
* 只要请求的块不超过512个页面(2K)，内核就尽量分配连续的页面
* 针对大内存分配设计

缺点：

* 合并的要求太过严格，只能是满足伙伴关系的块才能合并，比如第1块和第2块就不能合并。
* 碎片问题：一个连续的内存中仅仅一个页面被占用，导致整块内存区都不具备合并的条件
* 浪费问题：伙伴算法只能分配2的幂次方内存区，当需要8K（2页）时，好说，当需要9K时，那就需要分配16K（4页）的内存空间，但是实际只用到9K空间，多余的7K空间就被浪费掉。
* 算法的效率问题： 伙伴算法拆分和合并涉及了比较多的计算还有链表和位图的操作，开销还是比较大的，如果每次 $2^n$ 大小的伙伴块就会合并到  $2^{n+1}$ 的链表队列中，那么 $2^n$ 大小链表中的块就会因为合并操作而减少，但系统随后立即有可能又有对该大小块的需求，为此必须再从 $2^{n+1}$ 大小的链表中拆分，这样的合并又立即拆分的过程是无效率的。

**Linux针对大内存的物理地址分配**，采用伙伴算法，如果是针对小于一个page的内存，频繁的分配和释放，有更加适宜的解决方案，如slab和kmem_cache等



**优化?**

* 解决内部碎片过大问题（eg：申请5页，分配8页，浪费3页）：
	* ucore 在前部留下需要的页数，释放掉尾部各页。每次释放1页，先划分成页块，再逐个释放。
* 解决切分与合并过于频繁的问题：
	* 用得较多的是单个页。位于处理器Cache中页称为热页（hot page），其余页称为冷页（cold page）。处理器对热页的访问速度要快于冷页。可建一个热页队列（per_cpu_page），暂存刚释放的单个物理页，将合并工作向后推迟 Lazy。总是试图从热页队列中分配单个物理页。分配与释放都在热页队列的队头进行。
* 解决内存碎化(有足够多的空闲页，但是没有大页块)问题：
	* 将页块从一个物理位置移动到另一个物理位置，并保持移动前后逻辑地址不变（拷贝页块内容）
	* 逻辑内存管理器。
* 满足大内存的需求：

* 物理内存空间都耗尽的情况：
	* 在任何情况下，都应该预留一部分空闲的物理内存以备急需。定义两条基准线low和high，当空闲内存量小于low时，应立刻开始回收物理内存，直到空闲内存量大于high。

* 回收物理内存：
	* 法一：启动一个守护进程，专门用于回收物理内存。周期性启动，也可被唤醒。
	* 法二：申请者自己去回收内存。实际是由内存分配程序回收。回收的方法很多，如释放缓冲区、页面淘汰等。



#### Slab 算法？

slab是Linux操作系统的一种内存分配机制。其工作是针对一些**经常分配并释放的对象**，如进程描述符等，这些对象的**大小一般比较小**，如果直接采用伙伴系统来进行分配和释放，不仅会造成大量的内碎片，而且处理速度也太慢。而slab分配器是基于对象进行管理的，相同类型的对象归为一类(如进程描述符就是一类)，每当要申请这样一个对象，slab分配器就从一个slab列表中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统，从而避免这些内部碎片。slab分配器并不丢弃已分配的对象，而是释放并把它们保存在内存中。当以后又要请求新的对象时，就可以从内存直接获取而不用重复初始化。 


Linux 的slab 可有三种状态：

* 满的：slab 中的所有对象被标记为使用。
* 空的：slab 中的所有对象被标记为空闲。
* 部分：slab 中的对象有的被标记为使用，有的被标记为空闲。

slab 分配器首先从部分空闲的slab 进行分配。如没有，则从空的slab 进行分配。如没有，则从物理连续页上分配新的slab，并把它赋给一个cache ，然后再从新slab 分配空间。



与传统的内存管理模式相比， slab 缓存分配器提供了很多优点。
　　1、内核通常依赖于对小对象的分配，它们会在系统生命周期内进行无数次分配。
　　2、slab 缓存分配器通过对类似大小的对象进行缓存而提供这种功能，从而避免了常见的碎片问题。
　　3、slab 分配器还支持通用对象的初始化，从而避免了为同一目的而对一个对象重复进行初始化。
　　4、slab 分配器还可以支持硬件缓存对齐和着色，这允许不同缓存中的对象占用相同的缓存行，从而提高缓存的利用率并获得更好的性能。



### 非连续存储管理

#### 谈谈内存分段？

**什么是内存分段**

程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。**



**分段机制下，虚拟地址和物理地址是如何映射的？**

分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

![img](https://img-blog.csdnimg.cn/a9ed979e2ed8414f9828767592aadc21.png)

段选择因子和段内偏移量：

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。
- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

在上面，知道了虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：

![img](https://img-blog.csdnimg.cn/c5e2ab63e6ee4c8db575f3c7c9c85962.png)

如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。



**分段的优点**



**分段的缺点及解决方案**

分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：

- 第一个就是**内存碎片**的问题。
- 第二个就是**内存交换的效率低**的问题。

接下来，说说为什么会有这两个问题。

> 我们先来看看，分段为什么会产生内存碎片的问题？

我们来看看这样一个例子。假设有 1G 的物理内存，用户执行了多个程序，其中：

- 游戏占用了 512MB 内存
- 浏览器占用了 128MB 内存
- 音乐占用了 256 MB 内存。

这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。

如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。

![img](https://img-blog.csdnimg.cn/6142bc3c917e4a6298bdb62936e0d332.png)

> 内存分段会出现内存碎片吗？

内存碎片主要分为，内部内存碎片和外部内存碎片。

内存分段管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以**不会出现内部内存碎片**。

但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以**会出现外部内存碎片**的问题。

解决「外部内存碎片」的问题就是**内存交换**。

可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。

> 再来看看，分段为什么会导致内存交换效率低的问题？

对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。

因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。

所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

为了解决内存分段的「外部内存碎片和内存交换效率低」的问题，就出现了内存分页



#### 谈谈内存分页？

**什么是内存分页**

分段的好处就是能产生连续的内存空间，但是会出现「外部内存碎片和内存交换的空间太大」的问题。

要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

虚拟地址与物理地址之间通过**页表**来映射，如下图：

![img](https://img-blog.csdnimg.cn/08a8e315fedc4a858060db5cb4a654af.png)

页表是存储在内存里的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。

而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。



**分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？**

内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而**采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。**

但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对**内存分页机制会有内部内存碎片**的现象。

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**

![img](https://img-blog.csdnimg.cn/388a29f45fe947e5a49240e4eff13538.png)

更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是**只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。**





**分页机制下，虚拟地址和物理地址是如何映射的？**

在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。

![img](https://img-blog.csdnimg.cn/7884f4d8db4949f7a5bb4bbd0f452609.png)

总结一下，对于一个内存地址转换，其实就是这样三个步骤：

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：

![img](https://img-blog.csdnimg.cn/8f187878c809414ca2486b0b71e8880e.png)

这看起来似乎没什么毛病，但是放到实际中操作系统，这种简单的分页是肯定是会有问题的。



**分页的优点**



**分页的缺点及解决方案**

有空间上的缺陷。

因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。

在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 `4MB` 的内存来存储页表。

这 4MB 大小的页表，看起来也不是很大。但是要知道每个进程都是有自己的虚拟地址空间的，也就说都有自己的页表。

那么，`100` 个进程的话，就需要 `400MB` 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。



**多级页表**

要解决上面的问题，就需要采用一种叫作**多级页表**（*Multi-Level Page Table*）的解决方案。

在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 `4KB` 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。

我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**。如下图所示：

![img](https://img-blog.csdnimg.cn/19296e249b2240c29f9c52be70f611d5.png)

> 你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？

当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。

其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的**局部性原理**么？

每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。

如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= `0.804MB`，这对比单级页表的 `4MB` 是不是一个巨大的节约？

那么为什么不分级的页表就做不到这样节约内存呢？

我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。

我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。

对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%9B%9B%E7%BA%A7%E5%88%86%E9%A1%B5.png)



**TLB**

多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。

程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。

![img](https://img-blog.csdnimg.cn/edce58534d9342ff89f5261b1929c754.png)

我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（*Translation Lookaside Buffer*） ，通常称为页表缓存、转址旁路缓存、快表等。

![img](https://img-blog.csdnimg.cn/a3cdf27646b24614a64cfc5d7ccffa35.png)

在 CPU 芯片里面，封装了内存管理单元（*Memory Management Unit*）芯片，它用来完成地址转换和 TLB 的访问与交互。

有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。

TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。



#### 段页式内存管理？

内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为**段页式内存管理**。

![img](https://img-blog.csdnimg.cn/f19ebd6f70f84083b0d87cc5e9dea8e3.png)

段页式内存管理实现的方式：

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；

这样，地址结构就由**段号、段内页号和页内位移**三部分组成。

用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：

![img](https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png)

段页式地址变换中要得到物理地址须经过三次内存访问：

- 第一次访问段表，得到页表起始地址；
- 第二次访问页表，得到物理页号；
- 第三次将物理页号与页内位移组合，得到物理地址。

可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。





### 页面置换算法

#### 谈谈系统抖动与工作集

如果低优先级进程所分配的帧数低于计算机体系结构所需的最小数量，那么必须暂停该进程执行。然后，应调出它的所有剩余页面，以便释放所有分配的帧。这个规定引入了中级 CPU 调度的换进换出层。

事实上，需要研究一下没有“足够”帧的进程。如果进程没有需要支持活动使用页面的帧数，那么它会很快产生缺页错误。此时，必须置换某个页面。然而，由于它的所有页面都在使用中，所以必须立即置换需要再次使用的页面。因此，它会再次快速产生缺页错误，再一次置换必须立即返回的页面，如此快速进行。

这种高度的页面调度活动称为抖动。如果一个进程的调页时间多于它的执行时间，那么这个进程就在抖动。



**系统抖动的原因**

抖动导致严重的性能问题。考虑以下场景，这是基于早期调页系统的实际行为。

操作系统监视 CPU 利用率。如果 CPU 利用率太低，那么通过向系统引入新的进程来增加多道程度。采用全局置换算法会置换任何页面，而不管这些页面属于哪个进程。

现在假设进程在执行中进入一个新阶段，并且需要更多的帧。它开始出现缺页错误，并从其他进程那里获取帧。然而，这些进程也需要这些页面，因此它们也会出现缺页错误，并且从其他进程中获取帧。这些缺页错误进程必须使用调页设备以将页面换进和换出。当它们为调页设备排队时，就绪队列清空。随着进程等待调页设备，CPU 利用率会降低。

CPU 调度程序看到 CPU 利用率的降低，进而会增加多道程度。新进程试图从其他运行进程中获取帧来启动，从而导致更多的缺页错误和更长的调页设备队列。因此，CPU 利用率进一步下降，并且 CPU 调度程序试图再次增加多道程度。这样就出现了抖动，系统吞吐量陡降，缺页错误率显著增加。结果，有效内存访问时间增加，没有工作可以完成，因为进程总在忙于调页。

[![img](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-IOqZB0TN5f07263269538.gif)](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-IOqZB0TN5f07263269538.gif)
图 1 系统抖动

这种现象如图 1 所示，这里 CPU 利用率是按多道程度来绘制的。随着多道程度的增加，CPU 利用率也增加，虽然增加得更慢，直到达到最大值。如果多道程度还要进一步增加，那么系统抖动就开始了，并且 CPU 利用率急剧下降。此时，为了提高 CPU 利用率并停止抖动，必须降低多道程度。

通过局部置换算法或优先级置换算法，可以限制系统抖动。如果一个进程开始抖动，那么由于采用局部置换，它不能从另一个进程中获取帧，而且也不能导致后者抖动。然而，这个问题并没有完全解决。如果进程抖动，那么在大多数时间内会排队等待调页设备。由于调页设备的平均队列更长，缺页错误的平均等待时间也会增加。因此，即使对于不再抖动的进程，有效访问时间也会增加。

为了防止抖动，应为进程提供足够多的所需帧数。但是如何知道进程“需要”多少帧呢？有多种技术。工作集策略研究一个进程实际使用多少帧。这种方法定义了进程执行的局部性模型。

局部性模型指出，随着进程执行，它从一个局部移向另一个局部。局部性是最近使用页面的一个集合（图 2）。一个程序通常由多个不同的可能重叠的局部组成。

[![img](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-Sc3VCcce5f07263eced6b.gif)](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-Sc3VCcce5f07263eced6b.gif)
图 2 内存引用模式中的局部性

例如，当一个函数被调用时，它就定义了一个新的局部。在这个局部里，内存引用可针对函数调用的指令、它的局部变量以及全局变量的某个子集。当退出函数时，进程离开该局部，因为这个函数的局部变量和指令已不再处于活动使用状态。以后可能回到这个局部。

因此，可以看到局部是由程序结构和数据结构来定义的。局部性模型指出，所有程序都具有这种基本的内存引用结构。注意，局部性模型是目前为止缓存讨论的背后原理。如果对任何数据类型的访问是随机的而没有规律模式，那么缓存就没有用了。

假设为进程分配足够的帧以适应当前局部。该进程在其局部内会出现缺页错误，直到所有页面都在内存中；接着它不再会出现缺页错误，除非改变局部。如果没有能够分配到足够的帧来容纳当前局部，那么进程将会抖动，因为它不能在内存中保留正在使用的所有页面。

**工作集模型**

如上所述，工作集模型是基于局部性假设的。这个模型采用参数 △ 定义工作集窗口。它的思想是检查最近 △ 个页面引用。这最近 △ 个页面引用的页面集合称为工作集（如图 3 所示）。

[![img](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-KNJAFOxv5f07264c7c37a.gif)](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-KNJAFOxv5f07264c7c37a.gif)
图 3 工作集模型

如果一个页面处于活动使用状态，那么它处在工作集中。如果它不再使用，那么它在最后一次引用的 △ 时间单位后，会从工作集中删除。因此，工作集是程序局部的近似。

例如，给定如图 3 所示的内存引用序列，如果 △ 为 10 个内存引用，那么 t1 时的工作集为 {1，2, 5，6，7}。到 t2 时，工作集已经改变为 {3，4}。

工作集的精度取决于△的选择。如果 △ 太小，那么它不能包含整个局部；如果 △ 太大，那么它可能包含多个局部。在极端情况下，如果 △ 为无穷大，那么工作集为进程执行所需的所有页面的集合。

因此，最重要的工作集属性是它的大小。如果系统内的每个工作集通过计算为 WSS，那么就得到：
D=ΣWSSi

这里 D 为帧的总需求量。每个进程都使用其工作集内的页面。因此，进程 i 需要 WSSi 帧。如果总需求大于可用帧的总数（D>m），则将发生抖动，因此有些进程得不到足够的帧数。

一旦选中了 △，工作集模型的使用就很简单。操作系统监视每个进程的工作集，并为它分配大于其工作集的帧数。如果还有足够的额外帧，那么可启动另一进程。如果工作集大小的总和增加，以致超过可用帧的总数，则操作系统会选择一个进程来挂起。该进程的页面被写出（交换），并且其帧可分配给其他进程。挂起的进程以后可以重启。

这种工作集策略可防止抖动，同时保持尽可能高的多道程度。因此，它优化了 CPU 利用率。工作集模型的困难是跟踪工作集。工作集窗口是一个移动窗口。对于每次内存引用，新的引用出现在一端，最旧的引用离开另一端。如果一个页面在工作集窗口内的任何位置被引用过，那么它就在工作集窗口内。

通过定期时钟中断和引用位，我们能够近似工作集模型。

例如，假设 △ 为 10 000 个引用，而且每 5000 个引用引起定时器中断。当得到一个定时器中断时，复制并清除所有页面的引用位。如果发生缺页错误，那么可以检查当前的引用位和位于内存的两个位，这两位可以确定在过去的 10 000〜15 000 个引用之间该页面是否被使用过。如果使用过，那么这些位中至少有一位会被打开。如果没有使用过，那么这些位会被关闭。至少有一位打开的页面会被视为在工作集中。

注意，这种安排并不完全准确，这是因为并不知道在 5000 个引用内的什么位置出现了引用。通过增加历史位的数量和中断的频率（例如，10 位和每 1000 个引用中断一次），可以降低这一不确定性。然而，服务这些更为频繁中断的成本也会相应更高。

**缺页错误频率**

虽然工作集模型是成功的而且工作集的知识能够用于预先调页，但是用于控制抖动似乎有点笨拙。采用缺页错误频率（PFF）的策略是一种更为直接的方法。

这里的问题是防止抖动。抖动具有高缺页错误率。因此，需要控制缺页错误率。当缺页错误率太高时，我们知道该进程需要更多的帧。相反，如果缺页错误率太低，则该进程可能具有太多的帧。

[![img](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-K7Z1k6fU5f07265df03a1.gif)](http://bbs.yanzhishi.cn/image/show/attachments-2020-07-K7Z1k6fU5f07265df03a1.gif)
图 4 缺页错误频率

我们可以设置所需缺页错误率的上下限（图 4）。如果实际缺页错误率超过上限，则可为进程再分配一帧；如果实际缺页错误率低于下限，则可从进程中删除一帧。因此，可以直接测量和控制缺页错误率，以防止抖动。

与工作集策略一样，也可能不得不换出一个进程。如果缺页错误率增加并且没有空闲帧可用，那么必须选择某个进程并将其交换到后备存储。然后，再将释放的帧分配给具有高缺页错误率的进程。

实际上，抖动及其导致的交换对性能的负面影响很大。目前处理这一问题的最佳实践是，在可能的情况下提供足够物理内存以避免抖动和交换。从智能手机到大型机，提供足够内存，可以保持所有工作集都并发地处在内存中，并且提供最好的用户体验（除非在极端条件下）。



#### 谈谈 Belady 异常

在分页式虚拟存储器管理中，发生缺页时的置换算法采用**FIFO（先进先出）算法**时，如果对一个进程未分配它所要求的全部页面，有时就会出现**分配的页面数增多但缺页率反而提高**的异常现象。



#### 谈谈最佳页面置换算法 OPT

最佳页面置换算法基本思路是，**置换在「未来」最长时间不访问的页面**。

所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。

我们举个例子，假设一开始有 3 个空闲的物理页，然后有请求的页面序列，那它的置换过程如下图：

![最佳页面置换算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E6%9C%80%E4%BC%98%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png)

在这个请求的页面序列中，缺页共发生了 `7` 次（空闲页换入 3 次 + 最优页面置换 4 次），页面置换共发生了 `4` 次。

**这很理想，但是实际系统中无法实现**，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。

所以，最佳页面置换算法**作用是为了衡量你的算法的效率**，你的算法效率越接近该算法的效率，那么说明你的算法是高效的。





#### 谈谈先进先出置换算法 FIFO

既然我们无法预知页面在下一次访问前所需的等待时间，那我们可以**选择在内存驻留时间很长的页面进行中置换**，这个就是「先进先出置换」算法的思想。

还是以前面的请求的页面序列作为例子，假设使用先进先出置换算法，则过程如下图：

![先进先出置换算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/FIFO%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png)

在这个请求的页面序列中，缺页共发生了 `10` 次，页面置换共发生了 `7` 次，跟最佳页面置换算法比较起来，性能明显差了很多

* 没有考虑局部性原则：**它可能会踢出一个重要的页，而这个页马上要被引用**
* 是基于队列的，而不是堆栈，可能会有Belady 异常：当进程分配到的页面数增加的时候，缺页中断的次数可能增加也可能减少



#### 谈谈第二次机会页面置换算法 SC

  对FIFO算法的改进，对FIFO算法做一个简单的修改：检查最老页面的R位。如果R位是0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是1，就将R位置0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入时间使它就像刚装入的一样，然后继续搜索。
       第二次机会（second chance）算法是寻找一个在最近的时钟间隔内没有被访问过的页面。如果所有的页面都被访问过了，该算法就简化为纯粹的FIFO算法。假设所有页面的R位都被设置了，操作系统将会一个接一个地把每个页面都移动到链表的尾部并清除被移动的页面的R位。最后又会回到原来的表头页面，此时它的R位已经被清除了，因此这个页面会被淘汰，所以这个算法总是可以结束的。





#### 谈谈最近最久未使用的置换算法 LRU

最近最久未使用（*LRU*）的置换算法的基本思路是，发生缺页时，**选择最长时间没有被访问的页面进行置换**，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。

这种算法近似最优置换算法，最优置换算法是通过「未来」的使用情况来推测要淘汰的页面，而 LRU 则是通过「历史」的使用情况来推测要淘汰的页面。

还是以前面的请求的页面序列作为例子，假设使用最近最久未使用的置换算法，则过程如下图：

![最近最久未使用的置换算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/LRU%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png)

在这个请求的页面序列中，缺页共发生了 `9` 次，页面置换共发生了 `6` 次，跟先进先出置换算法比较起来，性能提高了一些。

虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。

困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。

所以，LRU 虽然看上去不错，但是由于开销比较大，实际应用中比较少使用。





#### 谈谈如何优化传统的 LRU（预读、缓存失效）

传统的 LRU 算法存在这两个问题：

- **「预读失效」导致缓存命中率下降（对应第一个题目）**
- **「缓存污染」导致缓存命中率下降（对应第二个题目）**



**什么是预读机制？**

Linux 操作系统为基于 Page Cache 的读缓存机制提供**预读机制**，一个例子是：

- 应用程序只想读取磁盘上文件 A 的 offset 为 0-3KB 范围内的数据，由于磁盘的基本读写单位为 block（4KB），于是操作系统至少会读 0-4KB 的内容，这恰好可以在一个 page 中装下。
- 但是操作系统出于空间局部性原理（靠近当前被访问数据的数据，在未来很大概率会被访问到），会选择将磁盘块 offset [4KB,8KB)、[8KB,12KB) 以及 [12KB,16KB) 都加载到内存，于是额外在内存中申请了 3 个 page；

下图代表了操作系统的预读机制：

![img](https://img-blog.csdnimg.cn/img_convert/ae8252378169c8c14b8b9907983f7d8b.png)

上图中，应用程序利用 read 系统调动读取 4KB 数据，实际上内核使用预读机制（ReadaHead） 机制完成了 16KB 数据的读取，也就是通过一次磁盘顺序读将多个 Page 数据装入 Page Cache。

这样下次读取 4KB 数据后面的数据的时候，就不用从磁盘读取了，直接在 Page Cache 即可命中数据。因此，预读机制带来的好处就是**减少了 磁盘 I/O 次数，提高系统磁盘 I/O 吞吐量**。

MySQL Innodb 存储引擎的 Buffer Pool 也有类似的预读机制，MySQL 从磁盘加载页时，会提前把它相邻的页一并加载进来，目的是为了减少磁盘 IO。



**预读失效会带来什么问题？**

如果**这些被提前加载进来的页，并没有被访问**，相当于这个预读工作是白做了，这个就是**预读失效**。

如果使用传统的 LRU 算法，就会把「预读页」放到 LRU 链表头部，而当内存空间不够的时候，还需要把末尾的页淘汰掉。

如果这些「预读页」如果一直不会被访问到，就会出现一个很奇怪的问题，**不会被访问的预读页却占用了 LRU 链表前排的位置，而末尾淘汰的页，可能是热点数据，这样就大大降低了缓存命中率** 。



**如何避免预读失效造成的影响**？

我们不能因为害怕预读失效，而将预读机制去掉，大部分情况下，空间局部性原理还是成立的。

要避免预读失效带来影响，最好就是**让预读页停留在内存里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在内存里的时间尽可能长**。

那到底怎么才能避免呢？

Linux 操作系统和 MySQL Innodb 通过改进传统 LRU 链表来避免预读失效带来的影响，具体的改进分别如下：

- Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active_list）和非活跃 LRU 链表（inactive_list）**；
- MySQL 的 Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域：**young 区域 和 old 区域**。

这两个改进方式，设计思想都是类似的，**都是将数据分为了冷数据和热数据，然后分别进行 LRU 算法**。不再像传统的 LRU 算法那样，所有数据都只用一个 LRU 算法管理。

接下来，具体聊聊 Linux 和 MySQL 是如何避免预读失效带来的影响？

> Linux 是如何避免预读失效带来的影响？

Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active_list）和非活跃 LRU 链表（inactive_list）**。

- **active list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；
- **inactive list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；

有了这两个 LRU 链表后，**预读页就只需要加入到 inactive list 区域的头部，当页被真正访问的时候，才将页插入 active list 的头部**。如果预读的页一直没有被访问，就会从 inactive list 移除，这样就不会影响 active list 中的热点数据。

接下来，给大家举个例子。

假设 active list 和 inactive list 的长度为 5，目前内存中已经有如下 10 个页：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BC%93%E5%AD%98/active_inactive_list.drawio.png)

现在有个编号为 20 的页被预读了，这个页只会被插入到 inactive list 的头部，而 inactive list 末尾的页（10号）会被淘汰掉。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BC%93%E5%AD%98/active_inactive_list1.drawio.png)

**即使编号为 20 的预读页一直不会被访问，它也没有占用到 active list 的位置**，而且还会比 active list 中的页更早被淘汰出去。

如果 20 号页被预读后，立刻被访问了，那么就会将它插入到 active list 的头部， active list 末尾的页（5号），会被**降级**到 inactive list ，作为 inactive list 的头部，这个过程并不会有数据被淘汰。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BC%93%E5%AD%98/active_inactive_list2.drawio.png)

> MySQL 是如何避免预读失效带来的影响？

MySQL 的 Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域，**young 区域 和 old 区域**。

young 区域在 LRU 链表的前半部分，old 区域则是在后半部分，这两个区域都有各自的头和尾节点，如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/young%2Bold.png)

young 区域与 old 区域在 LRU 链表中的占比关系并不是一比一的关系，而是是 7 比 3 （默认比例）的关系。

**划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部**。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。

接下来，给大家举个例子。

假设有一个长度为 10 的 LRU 链表，其中 young 区域占比 70 %，old 区域占比 30 %。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lrutwo.drawio.png)

现在有个编号为 20 的页被预读了，这个页只会被插入到 old 区域头部，而 old 区域末尾的页（10号）会被淘汰掉。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lrutwo2.png)

如果 20 号页一直不会被访问，它也没有占用到 young 区域的位置，而且还会比 young 区域的数据更早被淘汰出去。

如果 20 号页被预读后，立刻被访问了，那么就会将它插入到 young 区域的头部，young 区域末尾的页（7号），会被挤到 old 区域，作为 old 区域的头部，这个过程并不会有页被淘汰。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lrutwo3.png)



**什么是缓存污染？**

虽然 Linux （实现两个 LRU 链表）和 MySQL （划分两个区域）通过改进传统的 LRU 数据结构，避免了预读失效带来的影响。

但是如果还是使用「只要数据被访问一次，就将数据加入到活跃 LRU 链表头部（或者 young 区域）」这种方式的话，那么**还存在缓存污染的问题**。

当我们在批量读取数据的时候，由于数据被访问了一次，这些大量数据都会被加入到「活跃 LRU 链表」里，然后之前缓存在活跃 LRU 链表（或者 young 区域）里的热点数据全部都被淘汰了，**如果这些大量的数据在很长一段时间都不会被访问的话，那么整个活跃 LRU 链表（或者 young 区域）就被污染了**。



**缓存污染会带来什么问题**？

缓存污染带来的影响就是很致命的，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 I/O，系统性能就会急剧下降。

我以 MySQL 举例子，Linux 发生缓存污染的现象也是类似。

当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 I/O，MySQL 性能就会急剧下降。

注意， 缓存污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成缓存污染。

比如，在一个数据量非常大的表，执行了这条语句：

```sql
select * from t_user where name like "%xiaolin%";
```

可能这个查询出来的结果就几条记录，但是由于这条语句会发生索引失效，所以这个查询过程是全表扫描的，接着会发生如下的过程：

- 从磁盘读到的页加入到 LRU 链表的 old 区域头部；
- 当从页里读取行记录时，也就是**页被访问的时候，就要将该页放到 young 区域头部**；
- 接下来拿行记录的 name 字段和字符串 xiaolin 进行模糊匹配，如果符合条件，就加入到结果集里；
- 如此往复，直到扫描完表中的所有记录。

经过这一番折腾，由于这条 SQL 语句访问的页非常多，每访问一个页，都会将其加入 young 区域头部，那么**原本 young 区域的热点数据都会被替换掉，导致缓存命中率下降**。那些在批量扫描时，而被加入到 young 区域的页，如果在很长一段时间都不会再被访问的话，那么就污染了 young 区域。

举个例子，假设需要批量扫描：21，22，23，24，25 这五个页，这些页都会被逐一访问（读取页里的记录）。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lruthree.drawio.png)

在批量访问这些页的时候，会被逐一插入到 young 区域头部。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lruthree1.png)

可以看到，原本在 young 区域的 6 和 7 号页都被淘汰了，而批量扫描的页基本占满了 young 区域，如果这些页在很长一段时间都不会被访问，那么就对 young 区域造成了污染。

如果 6 和 7 号页是热点数据，那么在被淘汰后，后续有 SQL 再次读取 6 和 7 号页时，由于缓存未命中，就要从磁盘中读取了，降低了 MySQL 的性能，这就是缓存污染带来的影响。



**怎么避免缓存污染造成的影响**？

前面的 LRU 算法只要数据被访问一次，就将数据加入活跃 LRU 链表（或者 young 区域），**这种 LRU 算法进入活跃 LRU 链表的门槛太低了**！正式因为门槛太低，才导致在发生缓存污染的时候，很容就将原本在活跃 LRU 链表里的热点数据淘汰了。

所以，**只要我们提高进入到活跃 LRU 链表（或者 young 区域）的门槛，就能有效地保证活跃 LRU 链表（或者 young 区域）里的热点数据不会被轻易替换掉**。

Linux 操作系统和 MySQL Innodb 存储引擎分别是这样提高门槛的：

- **Linux 操作系统**：在内存页被访问**第二次**的时候，才将页从 inactive list 升级到 active list 里。

- MySQL Innodb

	：在内存页被访问

	第二次

	的时候，并不会马上将该页从 old 区域升级到 young 区域，因为还要进行

	停留在 old 区域的时间判断

	：

	- 如果第二次的访问时间与第一次访问的时间**在 1 秒内**（默认值），那么该页就**不会**被从 old 区域升级到 young 区域；
	- 如果第二次的访问时间与第一次访问的时间**超过 1 秒**，那么该页就**会**从 old 区域升级到 young 区域；

提高了进入活跃 LRU 链表（或者 young 区域）的门槛后，就很好了避免缓存污染带来的影响。

在批量读取数据时候，**如果这些大量数据只会被访问一次，那么它们就不会进入到活跃 LRU 链表（或者 young 区域）**，也就不会把热点数据淘汰，只会待在非活跃 LRU 链表（或者 old 区域）中，后续很快也会被淘汰。





#### 谈谈时钟页面置换算法 CLOCK （近似 LRU）

从计算开销的角度来看，**近似 LRU 更为可行，实际上这也是许多现代系统的做法**。这个想法需要硬件增加一个使用位（use bit，有时称为引用位，reference bit），这种做法在第一个支持分页的系统 Atlas one-level store 中实现。系统的每个页有一个使用位，然后这些使用位存储在某个地方（例如，它们可能在每个进程的页表中，或者只在某个数组中）。**每当页被引用（即读或写）时，硬件将使用位设置为 1**。但是，硬件不会清除该位（即将其设置为0），这由操作系统负责。

操作系统如何利用使用位来实现近似 LRU？可以有很多方法，有一个简单的方法称作**时钟算法**。想象一下，系统中的所有页都放在一个循环列表中。时钟指针开始时指向某个特定的页（哪个页不重要）。**当必须进行页替换时，操作系统检查当前指向的页 P 的使用位是 1 还是 0。如果是 1，则意味着页面 P 最近被使用，因此不适合被替换。然后，P 的使用位设置为0，时钟指针递增到下一页（P + 1）。该算法一直持续到找到一个使用位为 0 的页**。**使用位为0意味着这个页最近没有被使用过**（在最坏的情况下，所有的页都已经被使用了，那么就将所有页的使用位都设置为 0）。

请注意，这种方法不是通过使用位来实现近似 LRU 的唯一方法。实际上，任何**周期性地清除使用位，然后通过区分使用位是 1 和 0 来判定该替换哪个页的方法都是可以的**。Corbato 的时钟算法只是一个早期成熟的算法，并且具有不重复扫描内存来寻找未使用页的特点，也就是它在最差情况下，只会遍历一次所有内存。

下图展示了时钟算法的一个变种的行为。该变种在需要进行页替换时随机扫描各页，如果遇到一个页的引用位为 1，就清除该位（即将它设置为 0）。直到找到一个使用位为 0 的页，将这个页进行替换。如你所见，虽然**时钟算法不如完美的 LRU 做得好，但它比不考虑历史访问的方法要好**。

![img](https://pic.taifua.com/Picture/website/ostep/c22/9.png)



那有没有一种即能优化置换的次数，也能方便实现的算法呢？

时钟页面置换算法就可以两者兼得，它跟 LRU 近似，又是对 FIFO 的一种改进。

该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。

当发生缺页中断时，算法首先检查表针指向的页面：

- 如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
- 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

我画了一副时钟页面置换算法的工作流程图，你可以在下方看到：

![时钟页面置换算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E6%97%B6%E9%92%9F%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png)

了解了这个算法的工作方式，就明白为什么它被称为时钟（*Clock*）算法了。



#### 谈谈最不常用置换算法 LFU/NEU

最不常用（*LFU*）算法，这名字听起来很调皮，但是它的意思不是指这个算法不常用，而是**当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰**。

它的实现方式是，对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。

看起来很简单，每个页面加一个计数器就可以实现了，但是在操作系统中实现的时候，我们需要考虑效率和硬件成本的。

要增加一个计数器来实现，这个硬件成本是比较高的，另外如果要对这个计数器查找哪个页面访问次数最小，查找链表本身，如果链表长度很大，是非常耗时的，效率不高。

但还有个问题，LFU 算法只考虑了频率问题，没考虑时间的问题，比如有些页面在过去时间里访问的频率很高，但是现在已经没有访问了，而当前频繁访问的页面由于没有这些页面访问的次数高，在发生缺页中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不高的页面。

那这个问题的解决的办法还是有的，可以定期减少访问的次数，比如当发生时间中断时，把过去时间访问的页面的访问次数除以 2，也就说，随着时间的流失，以前的高访问次数的页面会慢慢减少，相当于加大了被置换的概率。



#### 谈谈老化算法

在NFU/LFU 基础上加入频度老化，实现了“最近”，越久的使用次数权重越

   老化算法是对LFU算法的修改，其修改包括两个部分，首先，在R位被加进之前将计数器右移一位，其次，将R位加到计数器最左端的位而不是最右端的位。
       老化算法中的计数器只有有限位数，如果时钟滴答是20ms，8位一般是够用的。假如一个页面160ms没有被访问过，那么它很可能并不重要。



#### 谈谈工作集页面置换算法

由于不可能精确地确定哪个页面是最近最少使用的，那就干脆不用花费这个力气，只维持少量的信息使得我们选出的替换页面不太可能是马上又会使用的页面即可。这种少量的信息就是工作集信息。

工作集概念来源于程序访问的时空局域性。即在一段时间内，程序访问的页面将局限在一组页面集合上。例如，最近k次访问均发生在某m个页面上，那么m就是参数为k时的工作集。我们用w（k，t）来表示在时间t时k次访问所涉及的页面数量。显然，随着k的增长，w（k，t）的值也随着增长；但在k增长到某个数值后，w（k，t）的值将增长极其缓慢甚至接近停滞，并维持一段时间的稳定。

![img](https:////upload-images.jianshu.io/upload_images/13572136-9db4c4d0ad5388a2.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1152/format/webp)

由此可以看出，如果一个程序在内存里面的页面数与其工作集大小相等或超过工作集，则该程序可在一段时间内不会发生缺页中断。如果其在内存的页面数小于工作集，则发生缺页中断的频率将增加，甚至发生内存抖动。

工作集算法的实现如下：

为页表的每个记录增加一项信息用来记录该页面最后一次被访问的时间。这个时间不必是真实时间，只要是按规律递增的一个虚拟时间即可。同时我们设置一个时间值T，如果一个页面的最后一次访问在当前时间减去T之前，则视为在工作集外，否则视为在工作集内。

**基于工作集的页面置换算法就是找出一个不在工作集中的页面并淘汰它。每个表项至少包含两条信息：上次使用该页面的近似时间和R（访问位）。**

过程：扫描所有的页面检查R位：

* 若（R == 1）：设置上次使用时间为当前实际时间，以表示缺页中断时该页面正在被使用
* 若（R == 0 且生存时间>τ）：移出这个页面，该页面在当前时钟滴答中未被访问，不在工作集中，用新的页面置换它。扫描会继续进行以更新剩余的表项。
* 若（R == 0 且生存时间≤τ）：记住最小时间。如果该页面R==0且生存时间小于或等于τ，则页面仍在工作集中。把页面临时保存下来，但是要记住生存时间最长（“上次使用时间”的最小值）。如果扫描完整个页表却没有找到合适的淘汰的页面，如果找到了一个或多个R == 0的页面，就淘汰生存时间最长的页面。

 在最坏的情况下，在当前时钟滴答中，所有的页面都被访问过了，也就是所有的R都为1，因此就随机选择一个页面淘汰，如果有的话最好选一个干净页面。

![img](https:////upload-images.jianshu.io/upload_images/13572136-bbe5368d856e4a20.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/779/format/webp)

如果在所有页面扫描后没有找到一个被替换的页面，则所有页面中最后一次访问时间最早的页面将被替换。这也是第b步记录当前最小值的原因。

优点：

- 工作集算法的优点是实现简单，只需在页表的每个记录增加一个虚拟时间域即可。因此，其空间成本不大。
- 该时间域不是每次发生访问时都需要修改，而是在需要更换页面时，页面更换算法对其进行修改，因此其时间成本也不大。

缺点：

- 是每次扫描页面进行替换时，有可能需要扫描整个页表。而我们知道，并不是所有页面都在内存里，因此扫描过程中的一大部分时间将是无用功。
- 由于其数据结构是线性的，造成每次都按同样的顺序进行扫描，这样就对某些页面不太公平。就好像评选优秀学生，如果大家的表现都同样优秀，就按照学号顺序来确定人选的话，那学号靠后的同学总是吃亏。





#### 工作集时钟页面置换算法

**在工作集页面置换算法中中，当缺页中断发生后，需要扫描整个页表才能确定被淘汰的页面，因此基本工作集算法是比较费时的。**

基于时钟算法，并且使用了工作集信息，被称为WSClock（工作集时钟）算法。由于它实现简单，性能较好，所以在实际工作中得到了广泛应用。与时钟算法一样，所需的数据结构是一个以页框为元素的循环表。最初，该表示空的，当装入第一个页面后，把它加到该表中。随着更多的页面加入，它们形成一个环。每个表项包含来自基本工作集算法的上次使用时间，以及R位和M位。

与时钟算法一样，每次缺页中断时，首先检查指针指向的页面。

* 如果R位是1，该页面在当前时钟滴答中就被使用过，那么该页面就不适合被淘汰。然后把该页面的R位置为0，指针指向下一个页面，并重复该算法。
* 如果R位是0，查看生存时间，如果生存时间大于τ并且该页面是干净的，它就不在工作集中，而且在磁盘上它有一个有效的副本。申请此页框，并把新页面放在其中。如果该页面已经被修改过，就不立即申请此页框，为了避免由于调度写磁盘操作引起的进程切换，指针继续向前走，算法继续对下一个页面进行操作，有可能存在一个旧的而且干净的页面可以立即使用。

原则上，所有的页面都有可能因为磁盘I/O在某个时钟周期被调度，为了降低磁盘阻塞，需要设置一个限制，即最大只允许写回n个页面。一旦达到该限制，就不允许调度新的写操作。

指针经过一圈返回它的起点，有两种情况：至少调用了一次写操作，没有调用过写操作

* 对于第一种情况，执行了写操作的页面已经不是干净的了，置换遇到的第一个干净页面，这个页面不一定是第一个被调度写操作的页面，因为硬盘驱动程序为了优化性能可能已经把写操作重排序了。
* 对于第二种情况，所有的页面都在工作集中，否则将至少执行了一个写操作。由于缺乏额外的信息，一个简单的方法就是随便置换一个干净的页面来使用，扫描中需要记录干净页面的位置。如果不存在干净页面，就选定当前页面并把它协会磁盘。



工作集时钟算法，即使用工作集算法的原理，但是将页面的扫描顺序按照时钟的形式组织起来。

**这样每次需要替换页面时，从指针指向的页面开始扫描，从而达到更加公平的状态。而且，按时钟组织的页面只是在内存里的页面，在内存外的页面不放在时钟圈里，从而提高实现效率。**

鉴于其时间和空间上的优势，工作集时钟算法被大多数商业操作系统所采纳。



#### 谈谈随机算法

另一个替换策略是**随机**，**在内存满的时候它随机选择一个页进行替换**。随机具有类似于 FIFO 的属性。实现起来很简单，但是它在挑选替换哪个页时不够智能。看看随机策略在上面例子上的引用流程。

![img](https://pic.taifua.com/Picture/website/ostep/c22/3.png)

当然，随机的表现完全取决于多幸运（或不幸）。在上面的例子中，随机比 FIFO 好一点，比最优的差一点。事实上，可以运行数千次的随机实验，求得一个平均的结果。下图显示了 10000 次试验后随机策略的平均命中率，每次试验都有不同的随机种子。正如所看到的，有些时候（仅仅 40% 的概率）随机和最优策略一样好，在上述例子中，命中内存的次数是 6 次。有时候情况会更糟糕，只有2次或更少。**随机策略取决于当时的运气**。

![img](https://pic.taifua.com/Picture/website/ostep/c22/4.png)
